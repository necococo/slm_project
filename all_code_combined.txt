--- Directory: . ---

Skipping binary file: ./.DS_Store
--- File: ./config.py ---
# slm/config.py
# Why not: 設定をファイルごとに分散させると保守が難しくなるため、集中管理する。

from typing import Optional
import os

class ModelConfig:
    """
    How:
        Wave Network + RoPE + Wavelet変換のモデル構成を定義します。
        波形表現の次元数やレイヤ数、語彙サイズなどをここで指定します。

    Attributes:
        hidden_size: モデル内部の隠れ次元 (wave表現における1つの軸の長さ)
        num_layers: WaveBlock + RoPEレイヤの段数
        vocab_size: 語彙サイズ（トークナイザーから自動取得も可能）
        max_seq_len: 最大シーケンス長
        use_wavelet: Wavelet変換を使用するかどうか (True/False)
        wavelet_name: PyWaveletsで使用するWaveletの名称 (例: 'haar', 'db1', 'db2'など)
    """
    def __init__(
        self,
        hidden_size: int = 256,
        num_layers: int = 3,
        vocab_size: Optional[int] = None,  # トークナイザーから取得する場合はNone
        max_seq_len: int = 1024,
        use_wavelet: bool = False,
        wavelet_name: Optional[str] = None
    ) -> None:
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self._vocab_size = vocab_size
        self.max_seq_len = max_seq_len
        self.use_wavelet = use_wavelet
        self.wavelet_name = wavelet_name
        self.tokenizer = None  # トークナイザーを後から設定可能に
    
    @property
    def vocab_size(self) -> int:
        """
        語彙サイズを取得。トークナイザーが設定されている場合はそこから取得、
        なければ初期化時に設定された値を使用
        """
        if self.tokenizer is not None:
            if hasattr(self.tokenizer, 'sp') and hasattr(self.tokenizer.sp, 'get_piece_size'):
                return self.tokenizer.sp.get_piece_size()
            elif hasattr(self.tokenizer, 'vocab_size'):
                return self.tokenizer.vocab_size
            else:
                raise AttributeError("トークナイザーからvocab_sizeを取得できません")
        
        if self._vocab_size is None:
            raise ValueError("vocab_sizeが設定されておらず、トークナイザーも設定されていません")
        
        return self._vocab_size
    
    def set_tokenizer(self, tokenizer) -> None:
        """
        トークナイザーを設定し、語彙サイズを自動的に取得できるようにする
        """
        self.tokenizer = tokenizer

        
class TrainingConfig:
    """
    How:
        学習時のハイパーパラメータをまとめるクラスです。

    Attributes:
        batch_size: バッチサイズ
        accumulation_steps: Gradient Accumulationのステップ数
        learning_rate: 学習率
        max_steps: 学習の最大ステップ
        mlm_mask_ratio: MLMマスク率
    """
    def __init__(
        self,
        batch_size: int = 8,
        accumulation_steps: int = 2,
        learning_rate: float = 1e-4,
        max_steps: int = 10000,
        mlm_mask_ratio: float = 0.15
    ) -> None:
        self.batch_size = batch_size
        self.accumulation_steps = accumulation_steps
        self.learning_rate = learning_rate
        self.max_steps = max_steps
        self.mlm_mask_ratio = mlm_mask_ratio


class PathsConfig:
    """
    How:
        データや重み、その他の出力を保存するパスを管理します。

    Attributes:
        data_dir: データセットを保存するベースディレクトリ
        model_dir: 学習済みモデル(重みファイル)を保存するディレクトリ
        dataset_name: 使用するデータセット名（例："shunk031/JGLUE", "wikitext"）
        dataset_subset: データセットのサブセット（例："JSQuAD", "wikitext-103-raw-v1"）
    """
    def __init__(
        self,
        data_dir: str = "/content/drive/MyDrive/data",
        model_dir: str = "/content/drive/MyDrive/models",
        dataset_name: str = "shunk031/JGLUE",
        dataset_subset: str = "JSQuAD",
        tokenizer_name: str = "spm"  # または "nerdstash" など
    ) -> None:
        self.data_dir = data_dir
        self.model_dir = model_dir
        self.dataset_name = dataset_name
        self.dataset_subset = dataset_subset
        self.tokenizer_name = tokenizer_name
        
    @property
    def dataset_dir(self) -> str:
        """データセットの保存ディレクトリパスを返します"""
        return os.path.join(self.data_dir, self.dataset_name, self.dataset_subset)
        
    @property
    def tokenizer_path(self) -> str:
        """トークナイザーモデルのパスを返します"""
        return os.path.join(self.model_dir, "tokenizers", "tokenizer.model")
        
    @property
    def model_save_dir(self) -> str:
        """モデル保存ディレクトリを返します"""
        return os.path.join(self.model_dir, "checkpoints")


--- File: ./pyproject.toml ---
[build-system]
requires = ["setuptools>=75", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "slm"
version = "0.1.0"
description = "プロジェクトの説明"  # Why: プロジェクト概要
readme = "slm/README.md"
requires-python = "==3.11.11"  # How: ColabのPythonバージョンに合わせる
authors = [
    { name = "necococo", email = "kmochidadihcomk@gmail.com" }  
]
dependencies = [
    "torch",
    "numpy",
    "sentencepiece",
    "datasets",
    "pywavelets",
    "huggingface_hub",
    "pytest",
]

[tool.setuptools]
packages = ["slm"]

--- File: ./main.py ---
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# main.py
# google colabでの実行を想定しています。
#
# 手順：
#  1) CPUランタイムで  !python main.py
#     => データセットを読み込み・トークナイズし、.save_to_disk(...) して終了
#  2) ランタイムをGPUに切り替え  !python main.py
#     => 前処理済みデータを読み込み、WaveNetworkLMを学習

import os
import sys
import torch
from datetime import datetime
from datasets import load_dataset, load_from_disk
from transformers import AutoTokenizer

from slm.config import ModelConfig, TrainingConfig, PathsConfig
from slm.model import WaveNetworkLM
from slm.trainer import Trainer

def main():
    """
    Wave Network言語モデルの学習を実行するメインスクリプト。
      - CPU環境: データセットの前処理（トークナイズ）だけ行い終了
      - GPU環境: 前処理済みデータを読み込み、学習を実行
    """
    print(f"=== Wave Network言語モデル学習 開始: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")

    # シード設定
    seed = 42
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # デバイス設定
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # パス設定
    paths_config = PathsConfig()
    
    # モデル設定
    model_config = ModelConfig()
    
    # 学習設定
    training_config = TrainingConfig()
    
    # ディレクトリ準備
    os.makedirs(paths_config.data_dir, exist_ok=True)
    os.makedirs(paths_config.checkpoint_dir, exist_ok=True)
    os.makedirs(paths_config.log_dir, exist_ok=True)
    
    # "resume" 引数があれば途中再開
    resume_checkpoint = None
    if len(sys.argv) > 1 and sys.argv[1] == "resume":
        resume_checkpoint = os.path.join(paths_config.checkpoint_dir, "final_model.pt")
        print(f"チェックポイントから復元します: {resume_checkpoint}")
    
    try:
        # CPUモードで前処理
        if device.type == "cpu":
            print("=== CPU環境: データ前処理を行い、終了します ===")
            
            # 1) データ読み込み
            print(f"Loading dataset: {paths_config.dataset_name}/{paths_config.dataset_subset}")
            dataset = load_dataset(paths_config.dataset_name, paths_config.dataset_subset)
            raw_train = dataset["train"]
            
            # dev用に 0.1% だけsplit
            split_data = raw_train.train_test_split(test_size=0.001, seed=42)
            train_dataset = split_data["train"]
            valid_dataset = split_data["test"]
            print(f"Train size: {len(train_dataset)}")
            print(f"Valid size: {len(valid_dataset)}")
            
            # 2) Tokenizer
            tokenizer = AutoTokenizer.from_pretrained(paths_config.tokenizer_name)
            print("Tokenizer vocab_size:", len(tokenizer))
            
            # 3) 前処理 (トークナイズ) 関数
            def tokenize_fn(examples):
                return tokenizer(examples["text"], 
                                #  truncation=True, 
                                #  max_length=model_config.max_seq_len
                                 )
            
            print("Tokenizing train_dataset ...")
            train_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])
            print("Tokenizing valid_dataset ...")
            valid_dataset = valid_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])
            
            # 4) 保存
            print("前処理済みデータをディスクに保存します...")
            train_dataset.save_to_disk(os.path.join(paths_config.data_dir, "train_dataset"))
            valid_dataset.save_to_disk(os.path.join(paths_config.data_dir, "valid_dataset"))
            
            print("=== 前処理完了。ランタイムをGPUに切り替えて再実行してください。 ===")
            sys.exit(0)
        
        # GPUモード: 学習パート
        print("=== GPU環境: 学習を実行します ===")
        
        # 前処理済みデータをロード
        train_dataset = load_from_disk(os.path.join(paths_config.data_dir, "train_dataset"))
        valid_dataset = load_from_disk(os.path.join(paths_config.data_dir, "valid_dataset"))
        
        # Tokenizer読み込み
        tokenizer = AutoTokenizer.from_pretrained(paths_config.tokenizer_name)
        
        # model_configにトークナイザをセット (vocab_sizeなどを取得させるため)
        model_config.set_tokenizer(tokenizer)
        
        # WaveNetworkLM を初期化
        print("WaveNetworkLMを初期化します...")
        model = WaveNetworkLM(model_config)

        # チェックポイント再開
        if resume_checkpoint and os.path.exists(resume_checkpoint):
            from slm.utils import load_checkpoint
            load_checkpoint(resume_checkpoint, model)

        # トレーナー
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            valid_dataset=valid_dataset,
            training_config=training_config,
            device=device,
            paths_config=paths_config
        )
        
        # 1) MLM学習
        print("MLM学習を開始...")
        trainer.train_mlm()
        
        # 2) Diffusion Fine-tuning (不要なら0epochにする)
        if training_config.diffusion_epochs > 0:
            print("Diffusionファインチューニングを開始します...")
            trainer.train_diffusion()
        
        # チェックポイント保存
        final_model_path = os.path.join(paths_config.checkpoint_dir, "final_model.pt")
        trainer.save_checkpoint("final_model")
        print(f"モデルを保存しました: {final_model_path}")
        
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Trainerリソース解放
        if 'trainer' in locals():
            trainer.close()
        
        print(f"=== 処理完了: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")


if __name__ == "__main__":
    main()

--- File: ./reset_git.sh ---
#!/bin/bash

# 現在のディレクトリを確認
current_dir=$(pwd)
echo "現在のディレクトリ: $current_dir"

# リポジトリのルートディレクトリに移動
cd /Users/kj/slm || { echo "Error: ディレクトリが存在しません"; exit 1; }

# 現在のリポジトリをバックアップ
backup_dir="../slm_backup_$(date +%Y%m%d%H%M%S)"
echo "リポジトリをバックアップ: $backup_dir"
cp -r . "$backup_dir"

# Gitリポジトリを初期化
echo "Gitリポジトリを初期化します..."
rm -rf .git
git init

# .gitignoreファイルを作成
cat > .gitignore << EOL
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
.pytest_cache/
.slm/

.DS_Store

all_code_combined.txt
copilot_code_style.md
EOL

# すべてのファイルをステージング
git add .

# 初期コミット
git commit -m "Initial commit: Wave Network Language Model"

# 確認メッセージ
echo "Gitリポジトリを初期化し、現在のコードベースを初期コミットとしました。"
echo "リモートリポジトリを設定するには以下のようなコマンドを実行してください："
echo "git remote add origin <repository-url>"
echo "git push -u origin main"


--- Directory: ./slm ---

--- File: ./slm/diffusion.py ---
# slm/diffusion.py
# Why not: Large Language Diffusion Models / LlaDA論文を簡易再現する例

import torch
import torch.nn as nn
from typing import Optional

class SimpleTextDiffusion(nn.Module):
    """
    How:
        テキストのマスクノイズを注入し、拡散過程で復元させる簡易版。
        実際にはノイズスケジュールなどを工夫する必要がある。
        WaveNetworkLM互換のインターフェース: forwardでembeddingsを、
        get_classifier_weightsでclassifierを返す設計に統一。
    """

    def __init__(self, timesteps: int = 20, mask_token_id: int = 4, vocab_size: Optional[int] = None) -> None:
        super().__init__()
        self.timesteps = timesteps
        self.mask_token_id = mask_token_id
        self.vocab_size = vocab_size
        # 擬似的な分類器の重み (identity matrix)
        self.classifier = None

    def forward(
        self,
        model: nn.Module,
        input_ids: torch.Tensor,
        t: int
    ) -> torch.Tensor:
        """
        How:
            tステップに応じてマスク率を変化させ、モデルに通して埋め込みを得る。
            
        Returns:
            torch.Tensor: モデルからの埋め込み出力 (embeddings)
        """
        ratio = (t + 1) / self.timesteps * 0.5
        corrupted = self._mask_tokens(input_ids, ratio)
        
        # モデルを通して埋め込みを取得
        embeddings = model(corrupted)
        
        # デバイスとvocab_sizeを記録
        if self.classifier is None and self.vocab_size is None and hasattr(model, 'get_classifier_weights'):
            classifier = model.get_classifier_weights()
            self.vocab_size = classifier.size(0)
            
        return embeddings
    
    def get_classifier_weights(self) -> torch.Tensor:
        """
        How:
            分類器の重みを返します。diffusion modelの場合は
            Identity行列を返すことで、埋め込みをそのままロジットとして使用します。
            
        Returns:
            torch.Tensor: 分類器の重み（Identity行列）
            形状は(vocab_size, hidden_size)を想定
            trainer.pyで.t()で転置するため、ここでは転置しない
        """
        if self.classifier is None:
            if self.vocab_size is None:
                raise ValueError("vocab_sizeが設定されていません。forwardを一度実行するか、初期化時にvocab_sizeを指定してください。")
            device = next(self.parameters()).device if list(self.parameters()) else torch.device('cpu')
            # trainer.pyで転置するため、ここでは(vocab_size, vocab_size)の単位行列を返す
            self.classifier = torch.eye(self.vocab_size, device=device)
        return self.classifier

    def _mask_tokens(
        self,
        input_ids: torch.Tensor,
        ratio: float
    ) -> torch.Tensor:
        device = input_ids.device
        masked = input_ids.clone()
        rand = torch.rand(input_ids.shape, device=device)
        mask_pos = rand < ratio
        masked[mask_pos] = self.mask_token_id
        return masked




--- File: ./slm/config.py ---
# slm/config.py
# Why not: 設定をファイルごとに分散させると保守が難しくなるため、集中管理する。

from typing import Optional
import os

class ModelConfig:
    """
    How:
        Wave Network + RoPE + Wavelet変換のモデル構成を定義します。
        波形表現の次元数やレイヤ数、語彙サイズなどをここで指定します。

    Attributes:
        hidden_size: モデル内部の隠れ次元 (wave表現における1つの軸の長さ)
        num_layers: WaveBlock + RoPEレイヤの段数
        vocab_size: 語彙サイズ（トークナイザーから自動取得も可能）
        max_seq_len: 最大シーケンス長
        use_wavelet: Wavelet変換を使用するかどうか (True/False)
        wavelet_name: PyWaveletsで使用するWaveletの名称 (例: 'haar', 'db1', 'db2'など)
        
    """
    def __init__(
        self,
        hidden_size: int = 1024,
        num_layers: int = 3,
        vocab_size: Optional[int] = None,  # トークナイザーから取得する場合はNone
        max_seq_len: int = 2048,
        use_rope: bool = True,
        use_wavelet: bool = False,
        wavelet_name: Optional[str] = None
    ) -> None:
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self._vocab_size = vocab_size
        self.max_seq_len = max_seq_len
        self.use_rope = use_rope
        self.use_wavelet = use_wavelet
        self.wavelet_name = wavelet_name
        self.tokenizer = None  # トークナイザーを後から設定可能に
    
    @property
    def vocab_size(self) -> int:
        """
        語彙サイズを取得。トークナイザーが設定されている場合はそこから取得、
        なければ初期化時に設定された値を使用
        """
        if self.tokenizer is not None:
            if hasattr(self.tokenizer, 'sp') and hasattr(self.tokenizer.sp, 'get_piece_size'):
                return self.tokenizer.sp.get_piece_size()
            elif hasattr(self.tokenizer, 'vocab_size'):
                return self.tokenizer.vocab_size
            else:
                raise AttributeError("トークナイザーからvocab_sizeを取得できません")
        
        if self._vocab_size is None:
            raise ValueError("vocab_sizeが設定されておらず、トークナイザーも設定されていません")
        
        return self._vocab_size

    def set_tokenizer(self, tokenizer) -> None:
        self.tokenizer = tokenizer
        
class TrainingConfig:
    """
    How:
        トレーニング関連の設定を保持するクラス。
        学習率、バッチサイズ、エポック数などを指定。
    """
    def __init__(
        self,
        learning_rate: float = 1e-5, # 1e-4だと数値が不安定になりロスにnanがでる
        batch_size: int = 24,
        mlm_epochs: int = 3,
        diffusion_epochs: int = 0,
        weight_decay: float = 0.01,
        warmup_steps: int = 500,
        # 以下に新しい設定項目を追加
        use_amp: bool = True,  # 混合精度トレーニング
        use_gradient_checkpointing: bool = True,  # 勾配チェックポイント
        gradient_accumulation_steps: int = 1,  # 勾配累積ステップ数
        clip_grad_norm: Optional[float] = True,  # 勾配クリッピング
    ):
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.mlm_epochs = mlm_epochs
        self.diffusion_epochs = diffusion_epochs
        self.weight_decay = weight_decay
        self.warmup_steps = warmup_steps
        # メモリ効率的な学習の設定
        self.use_amp = use_amp
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.clip_grad_norm = clip_grad_norm


class PathsConfig:
    """
    How:
        データや重み、その他の出力を保存するパスを管理します。

    Attributes:
        data_dir: データセットを保存するベースディレクトリ
        checkpoint_dir: 学習済みモデル(重みファイル)を保存するディレクトリ
        log_dir: TensorBoardログや学習進捗を保存するディレクトリ
        dataset_name: 使用するデータセット名（例："shunk031/JGLUE", "wikitext"）
        dataset_subset: データセットのサブセット（例："JSQuAD", "wikitext-103-raw-v1"）
    """
    def __init__(
        self,
        base_dir: str = "/content/drive/MyDrive/slm",
        dataset_name: str = "singletongue/wikipedia-utils",
        dataset_subset: Optional[str] = "corpus-jawiki-20230403-filtered-large",
        # tokenizer_name: str = "NovelAI/nerdstash-tokenizer-v2" 
        tokenizer_name: str = "cl-tohoku/bert-base-japanese-whole-word-masking"
        
    ) -> None:
        self.base_dir = base_dir
        self.data_dir = os.path.join(self.base_dir, "data")
        self.checkpoint_dir = os.path.join(self.base_dir, "checkpoints")
        self.log_dir = os.path.join(self.base_dir, "logs")
        self.dataset_name = dataset_name
        self.dataset_subset = dataset_subset
        self.tokenizer_name = tokenizer_name
        
    @property
    def dataset_dir(self) -> str:
        """データセットの保存ディレクトリパスを返します"""
        if self.dataset_subset:
            return os.path.join(self.data_dir, self.dataset_name, self.dataset_subset)
        return os.path.join(self.data_dir, self.dataset_name)
        
    @property
    def tokenizer_path(self) -> str:
        """トークナイザーモデルのパスを返します"""
        return os.path.join(self.checkpoint_dir, "tokenizers", "tokenizer.model")
        
    @property
    def model_save_dir(self) -> str:
        """モデル保存ディレクトリを返します"""
        return self.checkpoint_dir
        
    @property
    def tensorboard_log_dir(self) -> str:
        """TensorBoardのログディレクトリを返します"""
        return os.path.join(self.log_dir, "tensorboard")


--- File: ./slm/requirements.txt ---
sentencepiece
pywavelets
datasets
huggingface_hub
pytest
cut-cross-entropy
transformers
bitsandbytes
fugashi
ipadic
unidic-lite

--- File: ./slm/collator.py ---
# filename: collator.py

from typing import Dict, Optional, Tuple, Union
import torch

class CustomCollator:
    """
    How:
        - トークン列を DataLoader から受け取り、最大シーケンス長 (config.ModelConfig.max_seq_len)
          を超えるものはトリミング、満たない場合はパディングする。
        - MLM の場合はマスク処理を行う。
        - QA タスクの場合はQAラベルを処理する（今回省略）。
    """
    def __init__(
        self,
        tokenizer,
        model_config,  # ModelConfig を渡す
        mlm: bool = False,
        mlm_probability: float = 0.15,
        mask_token_id: Optional[int] = None,
        qa: bool = False
    ):
        self.tokenizer = tokenizer
        self.model_config = model_config
        self.mlm = mlm
        self.mlm_probability = mlm_probability
        self.qa = qa

        if mask_token_id is not None:
            self.mask_token_id = mask_token_id
        else:
            if tokenizer.mask_token_id is not None:
                self.mask_token_id = tokenizer.mask_token_id
            else:
                tmp_id = tokenizer.convert_tokens_to_ids("[MASK]")
                if tmp_id == tokenizer.unk_token_id:
                    print("[WARNING] [MASK] is not in the vocab. Using unk_token_id as mask.")
                    self.mask_token_id = tokenizer.unk_token_id
                else:
                    self.mask_token_id = tmp_id

    def __call__(self, batch):
        """
        How:
            - batch(list[dict]) から input_ids / labels を取り出し、
              ModelConfig.max_seq_len でトリミング or パディング。
            - mlm=True の場合は確率的にマスクトークンに置換し、labelsを-100で埋める。
        """
        # Why not: バッチ内で統一的に max_seq_len を取得して使う
        max_len = self.model_config.max_seq_len
        
        input_ids_list = []
        
        for sample in batch:
            # 取り出し
            input_ids = sample["input_ids"]
            # QAなどがある場合は別の処理かもしれないが、とりあえず同様にラベル扱い
            
            # 1) トリミング
            if len(input_ids) > max_len:
                input_ids = input_ids[:max_len]

            input_ids_list.append(input_ids)
        
        # 2) パディング
        #   バッチ内の max_len は model_config.max_seq_len と固定
        for i in range(len(input_ids_list)):
            diff = max_len - len(input_ids_list[i])
            if diff > 0:
                # 短い場合はパディング
                input_ids_list[i] += [self.tokenizer.pad_token_id] * diff

        # list -> tensor
        input_ids_batch = torch.tensor(input_ids_list, dtype=torch.long)
        labels_batch = input_ids_batch.clone()

        # 3) MLMマスク処理（mlm=True のときだけ）
        if self.mlm:
            probability_matrix = torch.full(labels_batch.shape, self.mlm_probability)
            special_ids = set([
                self.tokenizer.cls_token_id,
                self.tokenizer.sep_token_id,
                self.tokenizer.unk_token_id,
                self.tokenizer.pad_token_id
            ])
            mask_arr = torch.bernoulli(probability_matrix).bool()
            for s_id in special_ids:
                mask_arr &= (input_ids_batch != s_id)
            
            input_ids_batch[mask_arr] = self.mask_token_id
            labels_batch[~mask_arr] = -100

        return {
            "input_ids": input_ids_batch,
            "labels": labels_batch
        }

--- File: ./slm/evaluation.py ---
# slm/evaluation.py

"""
How:
    言語モデルの評価基準を複数導入します。
    - perplexity (PPL)
    - BLEU
    - ROUGE
    などを計算できるようにし、
    デコードには temperature を用いた確率サンプリングを実装。

Why not:
    Greedy だと多様性が乏しいため、temperature で出力分布を制御し、多様性を獲得。

What:
    evaluation.py から他のファイルで呼び出されることで、複数評価指標を統一的に計算可能。
"""

from typing import Optional
import torch
from torch.utils.data import DataLoader, Dataset
from slm.cce_loss import CceLoss
from slm.model import WaveHierarchicalLM

import evaluate

bleu_metric = evaluate.load("bleu")
rouge_metric = evaluate.load("rouge")


def evaluate_perplexity(
    model: WaveHierarchicalLM,
    dataset: Dataset,
    device: torch.device,
    batch_size: int = 8
) -> float:
    """
    How:
        クロスエントロピーからPerplexityを計算する。
    """
    model.eval()
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    criterion = CceLoss(ignore_index=-100)

    total_loss = 0.0
    total_tokens = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            labels = batch["labels"].to(device)
            logits = model(input_ids)
            loss_val = criterion(logits, labels) * labels.numel()
            total_loss += loss_val.item()
            total_tokens += labels.numel()

    avg_loss = total_loss / total_tokens
    ppl = torch.exp(torch.tensor(avg_loss))
    return ppl.item()


def evaluate_bleu(
    model: WaveHierarchicalLM,
    dataset: Dataset,
    device: torch.device,
    batch_size: int = 8,
    max_new_tokens: int = 32,
    temperature: float = 1.0
) -> float:
    """
    How:
        BLEUスコアを計測するための関数。
        dataset の各サンプルに 'target_text' があると想定し、
        temperature を使った確率サンプリングでモデル生成を行い比較。

    Why not:
        Greedy 生成では多様性が得られず BLEUスコアが一面的な評価に留まるため、
        temperature で分布を調整して評価する。

    What:
        - ここでは最小限の実装で、分かち書きや Tokenizer.decode() を簡易化している。
        - 本番環境では形態素解析や正規化処理が必要。
    """
    model.eval()
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    all_references = []
    all_hypotheses = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            # 参照文
            references = [[ref_str.split()] for ref_str in batch["target_text"]]

            outputs = []
            for i in range(input_ids.size(0)):
                single_input = input_ids[i].unsqueeze(0)
                generated = temperature_sampling_decode(
                    model,
                    single_input,
                    max_new_tokens,
                    device,
                    temperature=temperature
                )
                outputs.append(generated)

            # BLEU用に単語列にsplit
            hypotheses = [out_str.split() for out_str in outputs]

            all_references.extend(references)
            all_hypotheses.extend(hypotheses)

    results = bleu_metric.compute(
        predictions=all_hypotheses,
        references=all_references
    )
    return results["bleu"]


def evaluate_rouge(
    model: WaveHierarchicalLM,
    dataset: Dataset,
    device: torch.device,
    batch_size: int = 8,
    max_new_tokens: int = 32,
    temperature: float = 1.0
) -> dict:
    """
    How:
        ROUGEスコアを測るための関数。
        dataset の各サンプルに 'target_text' があると想定し、
        temperature サンプリングで生成したテキストを比較。

    Why not:
        Greedy だと確率最大トークン一辺倒になりがちなので、多様性を出すため temperature を活用。

    What:
        - ここでも単純なsplitを用いており、本格利用には正規化などが必要。
        - 戻り値は ROUGE の各種指標をまとめた辞書。
    """
    model.eval()
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    predictions = []
    references = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            ref_texts = batch["target_text"]

            gen_texts = []
            for i in range(input_ids.size(0)):
                single_input = input_ids[i].unsqueeze(0)
                generated = temperature_sampling_decode(
                    model,
                    single_input,
                    max_new_tokens,
                    device,
                    temperature=temperature
                )
                gen_texts.append(generated)

            predictions.extend(gen_texts)
            references.extend(ref_texts)

    results = rouge_metric.compute(
        predictions=predictions,
        references=references
    )
    return results


def temperature_sampling_decode(
    model: WaveHierarchicalLM,
    input_ids: torch.Tensor,
    max_new_tokens: int,
    device: torch.device,
    temperature: float = 1.0
) -> str:
    """
    How:
        temperature に基づいて、モデル出力ロジットをソフトマックス→1トークンをランダムサンプリングし、
        逐次生成を行う簡易関数。

    Why not:
        Greedy だと多様性が失われるため、temperature を使って確率分布を変形し、
        多様性を制御する。

    What:
        - 実際には Tokenizer が必要。ここでは ID列を str() で連結しているだけ。
        - top-k や top-p と組み合わせるとさらに良い。
    """
    model.eval()
    generated = input_ids.clone().to(device)

    for _ in range(max_new_tokens):
        logits = model(generated)  # (1, seq_len, vocab_size)
        next_token_logits = logits[:, -1, :]  # (1, vocab_size)

        # temperature スケーリング
        next_token_logits = next_token_logits / temperature

        probs = torch.softmax(next_token_logits, dim=-1)  # (1, vocab_size)
        # サンプリング
        next_token_id = torch.multinomial(probs, num_samples=1)  # (1,1)

        generated = torch.cat([generated, next_token_id], dim=1)

    # ID列をそのまま文字列に (本来はTokenizer.decode)
    tokens = generated.squeeze(0).cpu().tolist()
    out_str = " ".join(str(tk) for tk in tokens)
    return out_str


"""
このサンプルでは batch["target_text"] が存在する前提にしています。

QAタスク の場合は answers["text"] を参照としたり、翻訳 の場合は en_text → ja_text のようなペアを持ちます。
分かち書き や 句読点の扱い は評価指標によって調整してください。
greedy_decode() は非常に単純な実装です。本格的にやるなら transformers の generate() API (Beam Search, Top-k サンプリングなど) を利用し、出力を Tokenizer で decode する必要があります。
"""

--- File: ./slm/data_loader.py ---
# # data_loader.py
# import os
# from typing import Optional, Dict, Any, Tuple, Union
# from datasets import load_dataset, Dataset, DatasetDict
# from transformers import AutoTokenizer

# def tokenize_fn(example: Dict[str, Any], tokenizer, max_len: int) -> Dict[str, Any]:
#     """
#     How:
#         - QAの場合は question + context + answers をまとめる
#         - なければ example["text"] を使う
#         - Hugging FaceトークナイザでMLM用に 'input_ids','attention_mask','labels'を作る
#           (labelsは別途collatorで設定可能なので、ここではinput_idsだけでもよい)
#     """
#     if "question" in example and "context" in example and "answers" in example:
#         text = f"質問: {example['question']} 文脈: {example['context']} 答え: {example['answers']['text'][0]}"
#     elif "text" in example:
#         text = example["text"]
#     else:
#         text = str(example)

#     # トークナイザ呼び出し
#     encoded = tokenizer(
#         text,
#         truncation=True,
#         max_length=max_len,
#         # special_tokens => [CLS], [SEP] など自動付与
#         return_special_tokens_mask=True
#     )
#     return encoded


# def load_preprocessed_dataset(
#     dataset_name: str,
#     dataset_subset: Optional[str],
#     cache_dir: str,
#     tokenizer,
#     max_len: int = 512
# ) -> Tuple[Union[Dataset, None], Union[Dataset, None]]:
#     """
#     1. Hugging Faceからデータセットをロード
#     2. map(tokenize_fn) で前処理
#     3. train/validationを取り出して返す
#     """
#     print(f"=== データセット {dataset_name}/{dataset_subset} をロード＆前処理 ===")
#     ds = load_dataset(dataset_name, dataset_subset, cache_dir=cache_dir)

#     ds = ds.map(lambda ex: tokenize_fn(ex, tokenizer, max_len), batched=False)

#     # train/validation 分割を探す
#     if isinstance(ds, dict):  # DatasetDict
#         train_ds = ds.get("train", None)
#         valid_ds = ds.get("validation", ds.get("dev", None))
#     else:
#         train_ds = ds
#         valid_ds = None

#     return train_ds, valid_ds




--- File: ./slm/__init__.py ---
# """Small Language Model with wave network implementation."""

# from .config import PathsConfig, ModelConfig, TrainingConfig
# from .data_loader import download_dataset, load_and_preprocess_dataset, save_preprocessed_dataset
# from .tokenizer import JapaneseTokenizer
# from .cce_loss import CceLoss
# from .model import WaveHierarchicalLM
# from .train import train_model, resume_training
# from .evaluation import evaluate_model
# from .inference import generate_text
# from .utils import some_utility_function  # 必要な関数を追加

# __version__ = "0.1.0"

--- File: ./slm/model.py ---
# slm/model.py

from typing import Optional
import torch
import torch.nn as nn
import torch.nn.functional as F

from .modules.rope import RoPEEncoding  # Why not: ropeは外部モジュールとして利用する
from .config import ModelConfig
from .modules.wavelayer import SingleWaveLayer, to_wave_representation


class WaveNetworkLM(nn.Module):
    """
    How:
        - Embedding => Wave表現に変換
        - (option) RoPE on Wave representation (real, imag)
        - WaveNetwork (SingleWaveLayer×N)
        - output_proj (classifier weights) は持つが forward では logitsを作らず、埋め込みだけ返す。

    Why not:
        通常CEを使わず、cut-cross-entropy の linear_cross_entropy() のみを使うので、
        モデル自体は logits を出さずに済む。
    """

    def __init__(self, config: ModelConfig) -> None:
        super().__init__()
        self.config = config
        vocab_size = config.vocab_size

        # 埋め込み
        self.embedding = nn.Embedding(vocab_size, config.hidden_size)

        # RoPE (Wave表現に対して適用)
        self.rope = RoPEEncoding(config.hidden_size, config.max_seq_len) if config.use_rope else None

        # Wave blocks
        self.layers = nn.ModuleList([
            SingleWaveLayer(config.hidden_size)
            for _ in range(config.num_layers)
        ])

        # 分類器（cut-cross-entropy用の重み）
        self.classifier = nn.Linear(config.hidden_size, vocab_size, bias=False)

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        How:
            1) Embedding => (B,S,D)
            2) Wave表現に変換 => (real, imag)
            3) (option) RoPE => (real, imag)
            4) Wave layers => (B,S,D)
            5) return: (B,S,D)  (cut-cross-entropyの embedding)
        """
        x = self.embedding(input_ids)  # (B,S,D)

        # Wave表現に変換
        real, imag = to_wave_representation(x)

        # RoPE適用（オプション）
        if self.rope is not None:
            real, imag = self.rope(real, imag)  # Wave表現に対してRoPEを適用

        # SingleWaveLayerは実ベクトルを入出力するので、Wave表現からreal部分のみ取り出す、
        # または実部と虚部を結合して渡す実装に修正する必要があります
        # ここでは実装を簡略化するため、Wave表現から実ベクトルに戻します
        x = torch.sqrt(real**2 + imag**2)  # 振幅を計算

        for layer in self.layers:
            x = layer(x)  # (B,S,D)

        # 埋め込みのみを返す（cut-cross-entropyに必要）
        return x  # => embeddings (B,S,D)
    
    def get_classifier_weights(self) -> torch.Tensor:
        """
        How:
            cut-cross-entropyで linear_cross_entropy() を呼ぶ際に必要となる
            分類器の重み (V, D) を返す。
        """
        # classifier.weight shape: (V, D)
        return self.classifier.weight


--- File: ./slm/README.md ---



--- File: ./slm/tokenizer.py ---
# slm/tokenizer.py
# Why not: NovelAI/nerdstash-tokenizer-v2の組み込みも想定して構成をシンプルにしてある。

import os
import shutil
from typing import List, Optional
import sentencepiece as spm
from huggingface_hub import hf_hub_download

class JapaneseTokenizer:
    """
    How:
        日本語テキストをトークン化するクラスです。
        SentencePieceモデルファイルを指定して利用可能です、NovelAI/nerdstash-tokenizer-v2 なども利用できます。
    """

    def __init__(self, model_file: Optional[str] = None, hf_model: Optional[str] = None, 
                 save_to: Optional[str] = None) -> None:
        """
        Args:
            model_file: ローカルのSentencePieceモデルファイルパス
            hf_model: Hugging Faceのモデル名（例："NovelAI/nerdstash-tokenizer-v2"）
            save_to: ダウンロードしたモデルを保存するパス
        """
        if hf_model:
            # HuggingFaceからモデルをダウンロード
            model_path = hf_hub_download(repo_id=hf_model, filename="tokenizer.model")
            
            # 指定された場所に保存
            if save_to:
                os.makedirs(os.path.dirname(save_to), exist_ok=True)
                shutil.copy(model_path, save_to)
                print(f"モデルを保存しました: {save_to}")
                self.model_path = save_to
                self.sp = spm.SentencePieceProcessor(model_file=save_to)
            else:
                self.model_path = model_path
                self.sp = spm.SentencePieceProcessor(model_file=model_path)
        elif model_file:
            self.model_path = model_file
            self.sp = spm.SentencePieceProcessor(model_file=model_file)
        else:
            raise ValueError("model_fileまたはhf_modelを指定してください")

    def encode(self, text: str) -> List[int]:
        """
        How:
            テキストをID列に変換します。
        """
        return self.sp.encode(text, out_type=int)

    def decode(self, token_ids: List[int]) -> str:
        """
        How:
            ID列をテキストに変換します。
        """
        return self.sp.decode(token_ids)

    def tokenize_batch(self, texts: List[str]) -> List[List[int]]:
        """
        How:
            複数のテキストをまとめてトークン化します。
        """
        return [self.encode(t) for t in texts]
    
    @property
    def get_model_path(self) -> str:
        """モデルのパスを取得します"""
        return self.model_path
    



--- File: ./slm/setup.py ---
from setuptools import setup, find_packages

setup(
    name="slm",
    version="0.1.0",
    # 新フォルダ構成：コードが src/ 以下にある場合の例
    packages=find_packages(where="src"),
    package_dir={'': 'src'},
    install_requires=[
        "torch",
        "numpy",
        "sentencepiece",
        "pywavelets",
        "datasets",
        "huggingface_hub",
        "pytest",
    ],
)

--- File: ./slm/utils.py ---
# utils.py
import torch
import os
import torch.nn as nn
from typing import Tuple, Dict, Any, Optional, Union

def save_checkpoint(state: dict, checkpoint_dir: str, filename: str = None):
    """
    チェックポイントを保存する関数。
    
    Args:
        state (dict): モデル、オプティマイザ、その他情報を含む辞書。
        checkpoint_dir (str): チェックポイントを保存するディレクトリ。
        filename (str, optional): 保存するファイル名。指定がない場合は自動的に "checkpoint_epoch_{epoch}.pt" とします。
    """
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    if filename is None:
        epoch = state.get("epoch", 0)
        filename = f"checkpoint_epoch_{epoch}.pt"
    path = os.path.join(checkpoint_dir, filename)
    torch.save(state, path)
    print(f"Checkpoint saved to {path}")

def load_checkpoint(checkpoint_path: str, model, optimizer=None, device="cpu"):
    """
    保存されたチェックポイントからモデルとオプティマイザの状態を読み込む関数
    
    Args:
        checkpoint_path (str): 保存されたチェックポイントのパス。
        model (torch.nn.Module): 読み込み先のモデル。
        optimizer (torch.optim.Optimizer, optional): オプティマイザ（存在する場合）。
        device (str or torch.device, optional): モデルを配置するデバイス。
    
    Returns:
        int: チェックポイントに記録されたエポック番号（なければ0）。
    """
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    if optimizer is not None and "optimizer_state_dict" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    epoch = checkpoint.get("epoch", 0)
    print(f"Checkpoint loaded from {checkpoint_path}, epoch {epoch}")
    return epoch

def get_model_size(model: nn.Module) -> int:
    """
    モデルのパラメータ数（トレーニング可能な重みの数）を計算します。
    
    Args:
        model: サイズを計算するモデル
        
    Returns:
        パラメータの総数
    """
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def compute_flops_per_batch(model: nn.Module, input_shape: Tuple[int, ...], detailed: bool = False) -> Union[float, Dict[str, float]]:
    """
    モデルの1バッチあたりの概算FLOPsを計算します。
    
    実装の詳細:
        かなり大雑把な見積もりで、実際のFLOPsは実装によって大きく変わります。
        主要な行列積演算や活性化関数の計算量を基に推定値を出します。
        
    Args:
        model: FLOPsを計算するモデル
        input_shape: 入力テンソルの形状。通常は(batch_size, seq_len)
        detailed: Trueの場合は各レイヤー別の計算量も返す
        
    Returns:
        推定されるFLOPs数
    """
    # WaveHierarchicalLM向けの簡易FLOPs計算
    batch_size, seq_len = input_shape
    hidden_size = model.config.hidden_size if hasattr(model, 'config') else 256
    num_layers = model.config.num_layers if hasattr(model, 'config') else 3
    vocab_size = model.config.vocab_size if hasattr(model, 'config') else 30000
    
    # 各コンポーネントのFLOPs見積もり
    embedding_flops = batch_size * seq_len * hidden_size  # Embedding lookup
    
    # Wave表現への変換
    wave_conversion_flops = batch_size * seq_len * hidden_size * 5  # G計算+三角関数+乗算
    
    # WaveBlock計算 (各レイヤー)
    single_layer_flops = batch_size * seq_len * (
        # 線形変換x2
        2 * hidden_size * hidden_size * 2 * 2 +
        # 加算と波形変換
        hidden_size * 10
    )
    
    # RoPE適用（使用時）
    rope_flops = 0
    if hasattr(model, 'use_rope') and model.use_rope:
        rope_flops = batch_size * seq_len * hidden_size * 6  # cos/sin乗算
    
    # 出力層
    output_flops = batch_size * seq_len * hidden_size * 2 * vocab_size
    
    # 合計
    total_flops = (
        embedding_flops + 
        wave_conversion_flops + 
        single_layer_flops * num_layers +
        rope_flops * num_layers +
        output_flops
    )
    
    if detailed:
        return {
            'embedding': embedding_flops,
            'wave_conversion': wave_conversion_flops,
            'wave_blocks': single_layer_flops * num_layers,
            'rope': rope_flops * num_layers,
            'output': output_flops,
            'total': total_flops
        }
    
    return total_flops

def create_directory_for_path(path: str) -> None:
    """
    指定されたファイルパスのディレクトリが存在しない場合作成します。
    
    Args:
        path: ディレクトリを作成するファイルパス
    """
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

def get_device_info() -> Dict[str, Any]:
    """
    利用可能なデバイス情報を取得します。
    
    Returns:
        デバイス情報を含む辞書
    """
    info = {
        'cuda_available': torch.cuda.is_available(),
        'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
    }
    
    if info['cuda_available']:
        info['current_device'] = torch.cuda.current_device()
        info['device_name'] = torch.cuda.get_device_name(info['current_device'])
        info['memory_allocated'] = torch.cuda.memory_allocated(info['current_device'])
        info['memory_reserved'] = torch.cuda.memory_reserved(info['current_device'])
    
    return info

def setup_colab_for_a100() -> None:
    """
    Google Colabの環境でA100 GPUを使用するための最適な設定を行います。
    
    実装の詳細:
        - GPUメモリ使用の最適化
        - JITコンパイル設定
        - データローダーワーカー設定
    """
    # Google Colabでの設定
    try:
        import torch.backends.cudnn as cudnn
        cudnn.benchmark = True  # 入力サイズが一定の場合はBenchmarkモードでさらに高速化
        
        # 必要に応じてDataLoaderのワーカー数を調整
        import multiprocessing as mp
        torch.set_num_threads(mp.cpu_count() - 1)
        
        print("Google Colab環境のA100 GPU用に最適化設定を適用しました")
    except Exception as e:
        print(f"最適化設定の適用中にエラーが発生しました: {e}")

def log_to_tensorboard(writer, values: Dict[str, float], step: int, prefix: str = "") -> None:
    """
    複数のメトリクスをTensorBoardに一度に記録します。
    
    Args:
        writer: TensorBoardのSummaryWriterインスタンス
        values: 記録する値の辞書 {名前: 値}
        step: 現在のステップ（x軸）
        prefix: メトリクス名のプレフィックス
    """
    for name, value in values.items():
        metric_name = f"{prefix}/{name}" if prefix else name
        writer.add_scalar(metric_name, value, step)

def enable_amp_training(model: nn.Module, optimizer: torch.optim.Optimizer) -> Tuple[torch.cuda.amp.GradScaler, Any]:
    """
    混合精度トレーニングのセットアップを行います。
    
    Args:
        model: トレーニング対象のモデル
        optimizer: オプティマイザ
        
    Returns:
        Tuple: (GradScaler, model) AMP
    """
    scaler = torch.cuda.amp.GradScaler()
    model = torch.nn.parallel.DistributedDataParallel(model)
    return scaler, model




--- File: ./slm/compareLM.py ---
# slm/compareLM.py

"""
How:
    slm のモデル (ours) と Hugging Face のモデル (hf_model_name) を
    同じデータセットに対して評価し、比較する。

    ここでも temperature サンプリングを使って出力を生成・評価する方針に修正。

Why not:
    Greedy だと単調な出力になりやすい。多様性を評価するため temperature を考慮。

What:
    - PPL はモデル内部のloss計算なので特に temperature は関係ない
    - BLEU や ROUGE など生成タスクでは temperature で出力多様性を制御する
    - CSVに結果を保存する
"""

import os
import csv
import torch
from typing import Dict
from transformers import AutoTokenizer, AutoModelForCausalLM

from slm.model import WaveHierarchicalLM
from slm.evaluation import (
    evaluate_perplexity,
    evaluate_bleu,
    evaluate_rouge
)

def compare_two_models(
    ours: WaveHierarchicalLM,
    hf_model_name: str,
    dataset,
    device: torch.device,
    output_csv: str,
    temperature: float = 1.0
) -> None:
    """
    How:
        ours モデルと huggingface の hf_model_name をロードしたモデルを
        同一データで評価する。すべて temperature サンプリングで生成を行い、
        BLEUやROUGEを計測する。

    Why not:
        Greedy 生成を廃止し、temperature で多様性を制御する形に統一。

    What:
        - PPL は学習ログから計算されるため temperature は影響しない
        - BLEU, ROUGE は 生成出力に依存するので temperature を調整可能
    """
    # slm モデルの指標 (PPLはtemperature無関係)
    ours_ppl = evaluate_perplexity(ours, dataset, device)
    ours_bleu = evaluate_bleu(ours, dataset, device, temperature=temperature)
    ours_rouge = evaluate_rouge(ours, dataset, device, temperature=temperature)

    # Hugging Face model
    hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)
    hf_model = AutoModelForCausalLM.from_pretrained(hf_model_name)
    hf_model.to(device)

    hf_ppl = evaluate_perplexity_hf(hf_model, hf_tokenizer, dataset, device)
    hf_bleu = evaluate_bleu_hf(hf_model, hf_tokenizer, dataset, device, temperature=temperature)
    hf_rouge = evaluate_rouge_hf(hf_model, hf_tokenizer, dataset, device, temperature=temperature)

    # 表示
    print("=== Our Model ===")
    print(f"PPL:   {ours_ppl:.4f}")
    print(f"BLEU:  {ours_bleu:.4f}")
    print("ROUGE:", ours_rouge)

    print("=== HF Model: ", hf_model_name, "===")
    print(f"PPL:   {hf_ppl:.4f}")
    print(f"BLEU:  {hf_bleu:.4f}")
    print("ROUGE:", hf_rouge)

    # CSVに保存
    os.makedirs(os.path.dirname(output_csv), exist_ok=True)
    with open(output_csv, mode="w", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Model", "PPL", "BLEU", "ROUGE1", "ROUGE2", "ROUGEL"])
        writer.writerow([
            "WaveHierarchicalLM",
            f"{ours_ppl:.4f}",
            f"{ours_bleu:.4f}",
            f"{ours_rouge['rouge1']:.4f}",
            f"{ours_rouge['rouge2']:.4f}",
            f"{ours_rouge['rougeL']:.4f}"
        ])
        writer.writerow([
            hf_model_name,
            f"{hf_ppl:.4f}",
            f"{hf_bleu:.4f}",
            f"{hf_rouge['rouge1']:.4f}",
            f"{hf_rouge['rouge2']:.4f}",
            f"{hf_rouge['rougeL']:.4f}"
        ])
    print("結果をCSVに保存しました:", output_csv)


def evaluate_perplexity_hf(
    hf_model: AutoModelForCausalLM,
    hf_tokenizer: AutoTokenizer,
    dataset,
    device: torch.device,
    batch_size: int = 8
) -> float:
    """
    How:
        HFモデルに対して Perplexity を計測。
        temperature は PPL計算に影響しないため不要。

    Why not:
        PPLはモデル内部の確率(対数尤度)で計算されるため、生成戦略と独立。

    What:
        waveネットワークモデルとの計測手法を揃えるため、CrossEntropyLossベースで計算。
    """
    hf_model.eval()
    from torch.utils.data import DataLoader
    from slm.cce_loss import CceLoss
    import math

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    criterion = CceLoss(ignore_index=-100)

    total_loss = 0.0
    total_tokens = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            labels = batch["labels"].to(device)

            outputs = hf_model(input_ids, labels=labels)
            loss_val = outputs.loss * labels.numel()
            total_loss += loss_val.item()
            total_tokens += labels.numel()

    avg_loss = total_loss / total_tokens
    ppl = torch.exp(torch.tensor(avg_loss))
    return ppl.item()


def evaluate_bleu_hf(
    hf_model: AutoModelForCausalLM,
    hf_tokenizer: AutoTokenizer,
    dataset,
    device: torch.device,
    batch_size: int = 8,
    max_new_tokens: int = 32,
    temperature: float = 1.0
) -> float:
    """
    How:
        HFモデルで BLEU を測定。temperature を使ってサンプリングし、出力の多様性を制御。

    Why not:
        Greedy では単調になるので、temperature によるサンプリングで生成テキストを評価。

    What:
        - top-k, top-p と組み合わせればさらに多様性調整が可能。
        - 'target_text' がバッチに含まれている前提。
    """
    import evaluate
    from torch.utils.data import DataLoader

    bleu = evaluate.load("bleu")
    hf_model.eval()
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    all_references = []
    all_hypotheses = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids_list = batch["input_ids"]
            ref_texts = batch["target_text"]

            references = [[ref.split()] for ref in ref_texts]
            predictions = []
            for i, input_ids_raw in enumerate(input_ids_list):
                input_ids = input_ids_raw.unsqueeze(0).to(device)
                # temperature サンプリング
                gen_tokens = hf_model.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,           # サンプリングを有効化
                    temperature=temperature,  # ここで temperature を指定
                    num_beams=1
                )
                gen_str = hf_tokenizer.decode(gen_tokens[0], skip_special_tokens=True)
                predictions.append(gen_str)

            hypotheses = [p.split() for p in predictions]
            all_references.extend(references)
            all_hypotheses.extend(hypotheses)

    result = bleu.compute(
        predictions=all_hypotheses,
        references=all_references
    )
    return result["bleu"]


def evaluate_rouge_hf(
    hf_model: AutoModelForCausalLM,
    hf_tokenizer: AutoTokenizer,
    dataset,
    device: torch.device,
    batch_size: int = 8,
    max_new_tokens: int = 32,
    temperature: float = 1.0
) -> dict:
    """
    How:
        HFモデルで ROUGE を測定。temperature サンプリングで生成し評価。

    Why not:
        Greedy でなく分布サンプリングするので、多少変動が出るが多様性評価が可能。

    What:
        - ここでは簡易実装。top-k 等と組み合わせるのも一案。
        - 'target_text' を参照として想定。
    """
    import evaluate
    from torch.utils.data import DataLoader

    rouge = evaluate.load("rouge")
    hf_model.eval()
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    all_predictions = []
    all_references = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids_list = batch["input_ids"]
            ref_texts = batch["target_text"]

            predictions = []
            for i, input_ids_raw in enumerate(input_ids_list):
                input_ids = input_ids_raw.unsqueeze(0).to(device)
                gen_tokens = hf_model.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=temperature,
                    num_beams=1
                )
                gen_str = hf_tokenizer.decode(gen_tokens[0], skip_special_tokens=True)
                predictions.append(gen_str)

            all_predictions.extend(predictions)
            all_references.extend(ref_texts)

    return rouge.compute(
        predictions=all_predictions,
        references=all_references
    )

--- File: ./slm/inference.py ---
# slm/inference.py
# Why not: 簡単なGreedyサンプリング。QAとしての回答抽出ではない。

import torch
from slm.model import WaveHierarchicalLM

def sample_text(
    model: WaveHierarchicalLM,
    input_ids: torch.Tensor,
    max_len: int = 50,
    device: torch.device = torch.device("cpu")
) -> torch.Tensor:
    """
    How:
        Greedy方式で連続トークンを生成する簡易推論関数。
    """
    model.eval()
    generated = input_ids.clone().to(device)

    with torch.no_grad():
        for _ in range(max_len):
            logits = model(generated)  # (B, seq, vocab)
            next_token_logit = logits[:, -1, :]
            next_token = torch.argmax(next_token_logit, dim=-1).unsqueeze(-1)
            generated = torch.cat([generated, next_token], dim=1)

    return generated



--- File: ./slm/trainer.py ---
# slm/trainer.py
# Why not: 学習ループと評価を整理し、TensorBoard による監視を統合する

import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
from typing import Dict, Optional, Tuple, Union

from .config import ModelConfig, TrainingConfig, PathsConfig
from .model import WaveNetworkLM
from cut_cross_entropy import linear_cross_entropy
from .diffusion import SimpleTextDiffusion
from .utils import get_model_size, compute_flops_per_batch
from .collator import CustomCollator


class Trainer:
    """
    How:
        Wave Networkモデルの訓練を実行するトレーナークラス。
        TensorBoardを使った学習過程のモニタリングと、複数の学習フェーズ管理を行います。
    """

    def __init__(
        self,
        model: WaveNetworkLM,
        train_dataset: Dataset,
        valid_dataset: Optional[Dataset] = None,
        training_config: Optional[TrainingConfig] = None,
        paths_config: Optional[PathsConfig] = None,
        device: torch.device = None
    ):
        self.model = model
        self.train_dataset = train_dataset
        self.valid_dataset = valid_dataset
        self.training_config = training_config or TrainingConfig()
        self.paths_config = paths_config or PathsConfig()
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # モデルを通常のfloat32のままGPUへ (model.half()はしない)
        self.model.to(self.device)

        # ディレクトリ準備
        os.makedirs(self.paths_config.log_dir, exist_ok=True)
        os.makedirs(self.paths_config.checkpoint_dir, exist_ok=True)
        self.log_dir = self.paths_config.log_dir
        self.checkpoint_dir = self.paths_config.checkpoint_dir
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=self.paths_config.log_dir)
        
        # 学習率調整（NaN対策に小さめ）
        learning_rate = self.training_config.learning_rate
        if learning_rate > 1e-5:
            print(f"WARNING: Lowering learning rate from {learning_rate} to 1e-5 for stability")
            learning_rate = 1e-5

        # Optimizer
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=learning_rate,
            eps=1e-5  # epsilonを大きめに
        )
        
        # 追加: CosineAnnealingLR スケジューラの初期化
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=self.training_config.mlm_epochs,  # ここは必要に応じて調整
            eta_min=1e-6
        )
        
        # AMP用のGradScaler
        self.scaler = torch.amp.GradScaler('cuda') if (self.device.type == 'cuda' and self.training_config.use_amp) else None
        
        self._log_model_info()
        
    def _log_model_info(self):
        """モデルサイズなどをログ"""
        model_size = get_model_size(self.model)
        print(f"Model initialized with {model_size / 1e6:.2f}M parameters")
        print(f"Training on device: {self.device}")

    def train_mlm(self, num_epochs: Optional[int] = None) -> None:
        """
        MLM（Masked Language Model）方式で学習を実行
        """
        epochs = num_epochs or self.training_config.mlm_epochs
        
        tokenizer = self.model.config.tokenizer
        if tokenizer is None:
            raise ValueError("model.config.tokenizerが設定されていません。")

        # Collator
        collator = CustomCollator(
            tokenizer=tokenizer,
            model_config=self.model.config,
            mlm=True,
            mlm_probability=0.15,
            mask_token_id=tokenizer.mask_token_id,
            qa=False
        )
        
        dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.training_config.batch_size,
            shuffle=True,
            collate_fn=collator
        )
        
        total_steps = 0
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_start_time = time.time()
            
            for batch_idx, batch in enumerate(dataloader):
                step_loss = self._mlm_train_step(batch, total_steps)
                epoch_loss += step_loss
                total_steps += 1
                
                if batch_idx % 10 == 0:
                    print(f"Epoch {epoch+1}/{epochs} | Batch {batch_idx}/{len(dataloader)} | Loss: {step_loss:.4f}")
            
            # エポック統計
            avg_loss = epoch_loss / len(dataloader)
            epoch_time = time.time() - epoch_start_time
            print(f"Epoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.4f} | Time: {epoch_time:.2f}s")
            self.writer.add_scalar("MLM/Epoch Loss", avg_loss, epoch)
            self.writer.add_scalar("MLM/Epoch Time (s)", epoch_time, epoch)
            
            # エポック終了後にスケジューラを1ステップ進める
            self.scheduler.step()
            # 学習率を確認
            current_lr = self.optimizer.param_groups[0]["lr"]
            print(f"Current LR after scheduler step: {current_lr:.8f}")
            
            # Validation
            if self.valid_dataset:
                val_loss = self.validate()
                self.writer.add_scalar("MLM/Validation Loss", val_loss, epoch)
            
            # チェックポイント
            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:
                self.save_checkpoint(f"mlm_epoch_{epoch+1}")

        total_time = time.time() - start_time
        self.writer.add_scalar("MLM/Total Training Time (min)", total_time / 60, 0)
        print(f"MLM training completed in {total_time / 60:.2f} minutes")

    def _mlm_train_step(self, batch: Dict[str, torch.Tensor], step: int) -> float:
        self.model.train()
        input_ids = batch["input_ids"].to(self.device)
        labels = batch["labels"].to(self.device)

        self.optimizer.zero_grad()

        # forward
        embeddings = self.model(input_ids)  # => float32計算 + wave => 最終 .half()
        classifier = self.model.get_classifier_weights()  # => (V, D) float32かもしれないが...

        # cut_cross_entropy は embeddings, classifier が fp16 である必要があるので
        embeddings = embeddings.half()
        classifier = classifier.half()

        loss = linear_cross_entropy(embeddings, classifier, labels)
        loss.backward()
        self.optimizer.step()
            
        # ログ
        if step % 5 == 0:
            compute_time = 0.0  # 実測したい場合は time計測をどうぞ
            self.writer.add_scalar("MLM/Loss", loss.item(), step)
            self.writer.add_scalar("MLM/ComputeTime(ms)", compute_time * 1000, step)
            if torch.cuda.is_available():
                self.writer.add_scalar("System/GPU Memory (MB)", torch.cuda.memory_allocated()/1e6, step)
            est_flops = compute_flops_per_batch(self.model, input_ids.shape)
            self.writer.add_scalar("System/Estimated TFLOPS", est_flops/1e12/(compute_time+1e-9), step)
        
        return loss.item()

    def train_diffusion(self, num_epochs: Optional[int] = None) -> None:
        """
        拡散モデル方式でFine-tuning
        """
        epochs = num_epochs or self.training_config.diffusion_epochs
        if epochs == 0:
            print("Diffusion epochs = 0, skipping diffusion training")
            return

        vocab_size = self.model.get_classifier_weights().size(0)
        diffuser = SimpleTextDiffusion(timesteps=20, mask_token_id=4, vocab_size=vocab_size).to(self.device)
        
        dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.training_config.batch_size,
            shuffle=True
        )
        
        total_steps = 0
        start_time = time.time()

        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_start_time = time.time()
            
            for batch_idx, batch in enumerate(dataloader):
                t = torch.randint(0, diffuser.timesteps, (1,)).item()
                step_loss = self._diffusion_train_step(batch, diffuser, t, total_steps)
                epoch_loss += step_loss
                total_steps += 1

                if batch_idx % 10 == 0:
                    print(f"Diffusion Epoch {epoch+1}/{epochs} | Batch {batch_idx}/{len(dataloader)} | Loss: {step_loss:.4f}")

            avg_loss = epoch_loss / len(dataloader)
            epoch_time = time.time() - epoch_start_time
            print(f"Diffusion Epoch {epoch+1}/{epochs} done | Avg Loss: {avg_loss:.4f} | Time: {epoch_time:.2f}s")
            self.writer.add_scalar("Diffusion/Epoch Loss", avg_loss, epoch)
            self.writer.add_scalar("Diffusion/Epoch Time (s)", epoch_time, epoch)
            
            # エポック終了後にスケジューラを1ステップ進める
            self.scheduler.step()
            
            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:
                self.save_checkpoint(f"diffusion_epoch_{epoch+1}")
        
        total_time = time.time() - start_time
        self.writer.add_scalar("Diffusion/Total Training Time (min)", total_time / 60, 0)
        print(f"Diffusion training done in {total_time / 60:.2f} minutes")

    def _diffusion_train_step(self, batch: Dict[str, torch.Tensor], diffuser: SimpleTextDiffusion, t: int, step: int) -> float:
        self.model.train()
        input_ids = batch["input_ids"].to(self.device)
        labels = batch["labels"].to(self.device)
        
        self.optimizer.zero_grad()
        
        if self.scaler is not None:
            with torch.cuda.amp.autocast():
                embeddings = self.model(input_ids)
                classifier = self.model.get_classifier_weights()
                # ここで diffuser を使った計算が必要なら適宜足す
                loss = linear_cross_entropy(embeddings, classifier, labels)
            
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            embeddings = self.model(input_ids)
            classifier = self.model.get_classifier_weights()
            loss = linear_cross_entropy(embeddings, classifier, labels)
            
            loss.backward()
            self.optimizer.step()
        
        if step % 5 == 0:
            self.writer.add_scalar("Diffusion/Loss", loss.item(), step)
            self.writer.add_scalar("Diffusion/Timestep", t, step)
        
        return loss.item()

    def validate(self) -> float:
        if not self.valid_dataset:
            return 0.0
        self.model.eval()
        
        dataloader = DataLoader(
            self.valid_dataset,
            batch_size=self.training_config.batch_size,
            shuffle=False
        )
        
        total_loss = 0.0
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch["input_ids"].to(self.device)
                labels = batch["labels"].to(self.device)
                
                embeddings = self.model(input_ids)
                classifier = self.model.get_classifier_weights()
                # cut_cross_entropy は embeddings, classifier が fp16 である必要があるので
                embeddings = embeddings.half()
                classifier = classifier.half()
                loss = linear_cross_entropy(embeddings, classifier, labels)
                total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        print(f"Validation Loss: {avg_loss:.4f}")
        return avg_loss

    def save_checkpoint(self, name: str) -> None:
        checkpoint_path = os.path.join(self.checkpoint_dir, f"{name}.pt")
        checkpoint = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "model_config": self.model.config,
        }
        torch.save(checkpoint, checkpoint_path)
        print(f"Checkpoint saved to {checkpoint_path}")

    def load_checkpoint(self, path: str) -> None:
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        print(f"Model loaded from {path}")

    def close(self) -> None:
        self.writer.close()
        print("TensorBoard writer closed and resources released")

--- Directory: ./slm/modules ---

--- File: ./slm/modules/wavelayer.py ---
# wavelayer.py

from typing import Optional
import torch
import torch.nn as nn
import torch.nn.functional as F
from slm.config import ModelConfig

def to_wave_representation(x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
    """
    How:
        x (B,S,D) -> float32で安全に波形変換 => ratio => alpha => (real, imag)
        最終的には戻り値も float32 で返す（呼び出し側で必要に応じて .half() する）。
    """
    # Wave計算だけ float32
    x_32 = x.float()

    B, S, D = x_32.shape
    EPS = 1e-5
    
    # グローバル振幅
    G = torch.sqrt(torch.sum(x_32 * x_32, dim=1) + EPS)  # (B,D)
    G_exp = G.unsqueeze(1).expand(-1, S, -1)  # (B,S,D)
    G_safe = torch.clamp(G_exp, min=EPS)
    
    ratio = x_32 / G_safe
    ratio_clamped = torch.clamp(ratio, -0.99, 0.99)
    
    inside = 1.0 - ratio_clamped**2
    # 負値を ReLU でつぶし + EPS
    inside = F.relu(inside) + EPS

    alpha = torch.atan2(torch.sqrt(inside), ratio_clamped)
    real_part = G_safe * torch.cos(alpha)
    imag_part = G_safe * torch.sin(alpha)
    
    return real_part, imag_part  # float32


class SingleWaveLayer(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        dropout_prob: float = 0.1
    ) -> None:
        super().__init__()
        self.hidden_size = hidden_size

        # FeedForward
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size * 4),
            nn.ReLU(),
            nn.Linear(hidden_size * 4, hidden_size * 2)
        )
        self.ffn_norm = nn.LayerNorm(hidden_size * 2)

        # restore => (B,S,2D)->(B,S,D)
        self.restore_linear = nn.Linear(hidden_size * 2, hidden_size)
        self.overlay_dropout = nn.Dropout(dropout_prob)
        self.overlay_norm = nn.LayerNorm(hidden_size)

    def forward(
        self,
        x_embedding: torch.Tensor
    ) -> torch.Tensor:
        """
        How:
            - Wave変換を安全にfloat32で実行
            - 最終的な出力を .half() して返す
        """
        B, S, D = x_embedding.shape
        EPS = 1e-5
        
        # NaNチェック & float32化
        x_float = x_embedding.float()
        if torch.isnan(x_float).any():
            print("WARNING: NaN detected in layer input, replacing with 0.0")
            x_float = torch.nan_to_num(x_float, nan=0.0)

        # 1) 文レベルwave
        G_sen_scalar = torch.sqrt(torch.sum(x_float * x_float, dim=(1,2)) + EPS)
        G_sen_expanded = G_sen_scalar.view(B,1,1).expand(-1,S,D)
        G_sen_safe = torch.clamp(G_sen_expanded, min=EPS)
        
        ratio_sen = torch.clamp(x_float / G_sen_safe, -0.99, 0.99)
        inside_sen = F.relu(1.0 - ratio_sen**2) + EPS
        alpha_sen = torch.atan2(torch.sqrt(inside_sen), ratio_sen)
        real_sen = G_sen_safe * torch.cos(alpha_sen)
        imag_sen = G_sen_safe * torch.sin(alpha_sen)

        # 2) 入力wave
        real_in, imag_in = to_wave_representation(x_float)  # float32

        # 3) 干渉 (加算)
        out_real = real_sen + real_in
        out_imag = imag_sen + imag_in

        # amplitude
        amplitude = torch.sqrt(out_real**2 + out_imag**2 + EPS)
        sign = torch.sign(out_real + EPS)
        w_recon = amplitude * sign

        # 再wave
        real_w, imag_w = to_wave_representation(w_recon)

        # cat
        wave_cat = torch.cat([real_w, imag_w], dim=-1)  # (B,S,2D)

        # 4) FF + norm
        ff_out = self.ffn(wave_cat)
        wave_cat = self.ffn_norm(wave_cat + ff_out)

        # 5) restore => residual => dropout => norm
        restored = self.restore_linear(wave_cat)  # (B,S,D)
        # overlay: wave_cat の最初の D を residualとみなす例
        overlay_out = restored + wave_cat[..., :D]

        overlay_out = self.overlay_dropout(overlay_out)
        overlay_out = self.overlay_norm(overlay_out)

        # 最終的に half() で返す (cut_cross_entropy用)
        return overlay_out.half()

--- File: ./slm/modules/wavelet.py ---
import torch
import pywt

def apply_wavelet_transform(
    real: torch.Tensor,
    imag: torch.Tensor,
    wavelet_name: str
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    How:
        (real, imag) に対して簡単なWavelet変換を適用する例。
        実際には waveletGPT / WavSpA 論文のように複数スケールを扱うが、
        ここでは最小限の離散Wavelet変換(DWT)を適用し、低周波成分を復元して返すなど。

    Why not:
        本来は逆変換など複雑な操作が必要だが、デモとして最低限の実装。

    What:
        shape: (B,S,D)
        wavelet_name 例: "haar", "db1", "db2", ...
    """
    B, S, D = real.shape
    device = real.device

    real_out = torch.zeros_like(real)
    imag_out = torch.zeros_like(imag)

    for b in range(B):
        for d_ in range(D):
            signal_r = real[b, :, d_].cpu().numpy()
            signal_i = imag[b, :, d_].cpu().numpy()

            # 離散Wavelet変換
            cA_r, cD_r = pywt.dwt(signal_r, wavelet_name)
            cA_i, cD_i = pywt.dwt(signal_i, wavelet_name)

            # 簡易的に cA を 2倍に並べて元の長さに合わせる (実際は inverse dwt を使う)
            rec_r, rec_i = [], []
            for idx in range(len(cA_r)):
                rec_r.append(cA_r[idx])
                rec_r.append(cA_r[idx])
                rec_i.append(cA_i[idx])
                rec_i.append(cA_i[idx])
            
            # S を超えないようクリップ
            rec_r = rec_r[:S]
            rec_i = rec_i[:S]

            real_out[b, :, d_] = torch.tensor(rec_r, device=device)
            imag_out[b, :, d_] = torch.tensor(rec_i, device=device)

    return real_out, imag_out

--- File: ./slm/modules/rope.py ---
import torch
from torch import nn

class RoPEEncoding(nn.Module):
    """
    How:
        (real, imag) の複素ベクトルに RoPE を適用。
        real' = real*cos - imag*sin
        imag' = real*sin + imag*cos

    Why not:
        Wave Network と相性が良い実装方法として、(real,imag) に直接回転を掛ける。
        ここでは hidden_size次元 に対して pos依存の位相を乗せる。
    """

    def __init__(self, hidden_size: int, max_seq_len: int = 2048) -> None:
        super().__init__()
        self.hidden_size = hidden_size
        self.max_seq_len = max_seq_len

        freq_seq = torch.arange(0, hidden_size, dtype=torch.float32)
        freq_seq = 1.0 / (10000.0 ** (freq_seq / float(hidden_size)))
        pos_ids = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)
        theta = pos_ids * freq_seq.unsqueeze(0)  # (S, hidden_size)

        self.register_buffer("theta", theta, persistent=False)

    def forward(
        self,
        real: torch.Tensor,
        imag: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        How:
            (B,S,D) -> RoPE -> (B,S,D)
        """
        B, S, D = real.shape
        if S > self.max_seq_len:
            raise ValueError(f"シーケンス長 {S} が RoPE の {self.max_seq_len} を超過")

        theta_cur = self.theta[:S, :D].to(real.device)
        cos_part = torch.cos(theta_cur)
        sin_part = torch.sin(theta_cur)

        real_out = real * cos_part - imag * sin_part
        imag_out = real * sin_part + imag * cos_part

        return real_out, imag_out

