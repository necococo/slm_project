以下の論文を組み合わせた理論を考えたい。

[Wave Network: An Ultra-Small Language Model](https://arxiv.org/html/2411.02674v4)

### base
[Cut Your Losses in Large-Vocabulary Language Models](https://arxiv.org/html/2411.09009v1),(https://pypi.org/project/cut-cross-entropy/)
[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
### bit 
[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)
### probabilistic embeddings
[From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought](https://arxiv.org/pdf/2306.12672),(https://github.com/gabegrand/world-models)
[Exponential Family Attention](https://arxiv.org/html/2501.16790v1)
[WORD REPRESENTATIONS VIA GAUSSIAN EMBEDDING](https://arxiv.org/pdf/1412.6623)
[Informational Relative Evolution](https://huggingface.co/blog/Severian/informational-relative-evolution#67c327682430e43caa52d869)

### wavelet
[WaveletGPT: Wavelets Meet Large Language Models](https://arxiv.org/abs/2409.12924)
[WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability](https://arxiv.org/pdf/2210.01989)
### attention
[Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)
### fine-tuning
[Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models](https://arxiv.org/html/2502.13533v1)
[Transformer² : Self-adaptive LLMs](https://arxiv.org/abs/2501.06252)
[Low-Rank Adapters Meet Neural Architecture Search for LLM Compression](https://arxiv.org/abs/2501.16372)
### training 
[Large Language Diffusion Models](https://arxiv.org/pdf/2306.12672)
[Informational Relative Evolution](https://huggingface.co/blog/Severian/informational-relative-evolution#67c327682430e43caa52d869)
### test time
[Titans: Learning to Memorize at Test Time](https://arxiv.org/html/2501.00663v1) # google deepmild 長期記憶をもたせる
### with world model
[WorldGPT: Empowering LLM as Multimodal World Model](https://arxiv.org/html/2404.18202v2)
