wave networkをbaseに複層し、CCEで最終層の計算量を大幅に削減する。
日本語の前処理を経験できる。
新規のアーキテクチャの実装を経験できる。
CCEの適用を経験できる。
transformerと比較することでtransformerの理解を深められる。

| 精度などを比較する論文タイトル                                                       | リンク                                                     | 備考                                                  |
|--------------------------------------------------------------------|------------------------------------------------------------|-------------------------------------------------------|
| JGLUE: A Benchmark for Japanese Natural Language Understanding       | [https://arxiv.org/abs/2104.12210](https://arxiv.org/abs/2104.12210) | JSQuADを含む、日本語NLUの総合ベンチマーク                |
| JSQuAD: Japanese SQuAD                                               | [https://arxiv.org/abs/2106.01569](https://arxiv.org/abs/2106.01569) | SQuAD形式の日本語QAデータセット（Train: 63,870 / Dev: 4,475） |
| JAQKET: A Quiz-Style Japanese QA Dataset                             | [https://arxiv.org/abs/2004.08497](https://arxiv.org/abs/2004.08497) | クイズ形式QA、Wikipediaタイトルを正解とする              |

	•	学習データとしては、JSQuADのTrainセット（約63,870ペア）を使用し、Dev/Testセット（約4,475ペア程度）で評価するのが適しています。これにより、既存の日本語QA研究との比較が容易になります。
	•	前処理方法は、日本語BERTの先行事例に合わせ、MeCabで形態素解析しSentencePieceでサブワードトークナイズする手法を採用します。
	•	評価指標としては、Extractive QAの場合はExact MatchとF1スコア、Generative QAの場合は生成精度を測定する評価基準を用います。また、推論速度、パラメータ数、VRAM使用量なども比較対象となります。


今後の改良計画
	1.	基本実装の安定化と評価
	•	現在のコードが正しく動作するか、基本的な学習・推論サイクルが安定しているかを十分に検証する。
	•	単純なJSQuADなどのデータセットで基礎的な性能（損失、生成結果など）を確認する。
	2.	チェックポイント管理とテストコードの充実
	•	既存のsave/load機能や各モジュールのテストコードを整備して、バグフィックスや変更時のリグレッションテストを容易にする。
	•	コマンドライン引数や自動ファイル名生成など、運用面の改善を確実にする。
	3.	RoPEやCCEなど既存機能の最適化
	•	RoPEの効果が十分かどうか、ハイパーパラメータ（回転スケールなど）の調整を通じて性能向上を試みる。
	•	CCE損失の実装をさらにチューニングし、低確率項目のカット方法を改善して計算効率と学習安定性を図る。
	4.	Proxy Tokenの活用と階層構造の実装
	•	まずは現在のシンプルなプロキシトークンの挙動を精査し、その後、より明示的に文レベル・段落レベルなどの階層構造を導入する。
	•	多段階学習（Stage-wise: 単語レベル→文レベル→段落レベル）のスケジュールやネットワーク構造を設計し、層ごとに異なる役割を持たせる。
	5.	局所注意や周波数ゲートの補助モジュールの追加
	•	基本のWave Networkに加えて、局所的な文脈をより詳細に扱うための局所注意機構や、周波数ドメインでのゲート機構を追加する。
	•	これにより、細かい依存関係の捕捉と計算コスト削減の両立を目指す。
	6.	自己蒸留 (Self-Distillation) の導入
	•	計算負荷が少ない方法で自己蒸留を試行し、モデルの一般化能力の向上を図る。
	•	最初はオプションとして実装し、実験結果を見ながら導入の効果を評価する。
	7.	GPU/分散学習のさらなる最適化
	•	AccelerateやDeepSpeedの設定をさらにチューニングし、利用可能なGPU数に応じた最適な分散学習を実現する。
	•	必要に応じて、より高度なメモリ最適化技術（例えばZeROのステージ3など）を検討する。
	8.	マルチモーダル拡張（将来的な展開）
	•	まずはテキストモデルとして安定させた後、音声や画像、動画といった他のドメインにもWave Networkの枠組みを拡張する。
	•	これにより、ロボットの頭脳として複数モダリティの統合処理が可能なシステムへ発展させる。



Grok3: wave network cross attention
### 主要ポイント
- 研究は、Wave Networkの埋め込み表現が注意機構（attention）を代替可能である可能性を示唆していますが、完全な代替については不確実性が残ります。
- クロスアテンションを扱うための具体的な方法は論文に明記されていませんが、複素ベクトルの操作を拡張することで対応可能と考えられます。

---

### 埋め込み表現と注意機構の関係
Wave Networkは、トークンを複素ベクトルで表現し、波干渉（wave interference）や波変調（wave modulation）を通じてトークン間の相互作用を処理します。これにより、従来のTransformerモデルの注意機構を代替する可能性があるとされています。特に、AG Newsのテキスト分類タスクでは、Wave NetworkがBERTの単一Transformer層を上回る精度（90.91%〜91.66%対71.68%）を示し、リソース使用量も大幅に削減（VRAM使用量77.34%減、訓練時間85.62%減）しています。

---

### クロスアテンションの扱い方
Wave Networkは単一シーケンスの処理に特化しており、クロスアテンション（異なるシーケンス間の相互作用）の直接的な言及はありません。しかし、理論的には、別のシーケンスの複素ベクトルを利用して、位相ベクトルや大きさベクトルを調整することでクロスアテンションを実現できる可能性があります。例えば、シーケンスBのグローバルな意味情報をシーケンスAの複素ベクトルに組み込む方法が考えられますが、これは論文で具体的に提案されていません。

---

---

### 詳細な調査報告

#### はじめに
本報告書では、[Wave Network: An Ultra-Small Language Model](https://arxiv.org/abs/2411.02674)の論文に基づき、埋め込み表現が注意機構を代替可能かどうか、そしてクロスアテンションを扱う方法について詳しく調査します。Wave Networkは、従来のTransformerモデルと異なり、複素ベクトルを用いたトークン表現と波干渉・波変調による更新方法を特徴とします。これにより、リソース効率を高めつつ高い性能を実現する可能性が示唆されています。

#### Wave Networkの概要
Wave Networkは、トークンを複素ベクトル\(\mathbf{w} = \mathbf{G} \cdot e^{i \cdot \boldsymbol{\alpha}}\)で表現します。ここで、\(\mathbf{G}\)は入力テキストのグローバルな意味を表す大きさベクトル、\(\boldsymbol{\alpha}\)は個々のトークンとグローバルな意味との関係を捉える位相ベクトルです。この表現は、静的トークン埋め込み（例：Skip-gram、CBOW）やコンテキスト依存の埋め込み方法と比較され、効率的なトークン更新を可能にします。

論文では、波干渉（加算）と波変調（乗算）という2つの操作が提案されています。これらの操作は、トークン間の相互作用を処理し、注意機構の役割を代替する可能性があります。具体的には、AG Newsテキスト分類タスクでの実験結果では、Wave Networkの単一層がランダム初期化されたトークン埋め込みから90.91%（波干渉）および91.66%（波変調）の精度を達成し、BERTの事前訓練済み埋め込みを使用した単一Transformer層（71.68%）を19.23%〜19.98%上回りました。さらに、BERT baseモデル（94.64%）に近い精度を2.4Mパラメータの小規模モデルで実現し、ビデオメモリ使用量を77.34%減、訓練時間を85.62%短縮しました（[Wave Network: An Ultra-Small Language Model](https://arxiv.org/abs/2411.02674)参照）。

以下に、主要な実験結果を表形式でまとめます：

| タスク       | Wave Network (干渉) | Wave Network (変調) | Transformer (BERT埋め込み) | BERT base |
|--------------|---------------------|---------------------|----------------------------|-----------|
| AG News      | 90.91%             | 91.66%             | 71.68%                    | 94.64%    |
| DBpedia14    | 97.93%             | 98.05%             | -                         | 99.28%    |
| IMDB         | 87.00%             | 87.02%             | -                         | 93.94%    |

（出典：[Wave Network: An Ultra-Small Language Model](https://arxiv.org/html/2411.02674v4#S5.T2)）

#### 埋め込み表現が注意機構を代替可能か
注意機構（attention）は、Transformerモデルにおいてトークン間の依存関係を捉えるための鍵となる仕組みです。一方、Wave Networkでは、複素ベクトルの操作を通じて同様の機能を実現しようとしています。波干渉は複素ベクトルの加算、波変調は乗算を通じてトークン間の相互作用を更新し、これが注意機構の役割を代替する可能性があると論文は示唆します。特に、単一層での高い精度とリソース効率は、注意機構を必要としない新たなアプローチの有効性を示しています。

しかし、注意機構は多様なタスク（例：機械翻訳、要約生成）で必須とされる場合が多く、Wave Networkがすべてのシナリオで代替可能かどうかは不明です。論文の焦点はテキスト分類であり、長期依存関係や多シーケンス処理の評価は含まれていません。このため、埋め込み表現が注意機構を完全に代替できるかは、さらなる研究が必要です。

#### クロスアテンションの扱い方
クロスアテンションは、例えばエンコーダ・デコーダモデルにおいて、1つのシーケンスが別のシーケンスに注目する必要がある場合に使用されます。Wave Networkは単一シーケンスの処理に特化しており、論文内でクロスアテンションの直接的な言及はありません（[Wave Network: An Ultra-Small Language Model](https://arxiv.org/html/2411.02674v4)参照）。

しかし、理論的には、Wave Networkを拡張することでクロスアテンションを扱う方法が考えられます。例えば、シーケンスBの複素ベクトルをシーケンスAのグローバルな意味情報\(\mathbf{G}\)や位相ベクトル\(\boldsymbol{\alpha}\)に組み込む方法です。具体的には、シーケンスBの平均複素ベクトルでシーケンスAのトークンを変調する、またはシーケンスBの各トークンとの関係を位相ベクトルに反映させるなどのアプローチが考えられます。

このような拡張は、論文で提案されていないため、具体的な実装方法や性能は不明です。クロスアテンションを必要とするタスク（例：機械翻訳）では、Wave Networkの現在の形式では限界がある可能性があります。

#### 結論と今後の展望
Wave Networkの埋め込み表現は、テキスト分類タスクにおいて注意機構を代替する可能性を示していますが、クロスアテンションのような複雑な相互作用を扱うためには、モデルを拡張する必要があります。複素ベクトルの操作を活用した新たなアプローチは、リソース効率の高い言語モデルの開発に寄与する可能性がありますが、多シーケンス処理の能力についてはさらなる研究が必要です。

---

### 主要引用
- [Wave Network: An Ultra-Small Language Model](https://arxiv.org/abs/2411.02674)
- [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
- [Cross-layer Attention Sharing for Large Language Models](https://arxiv.org/abs/2408.01890)
- [Chapter 8 Attention and Self-Attention for NLP](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html)
<!-- [Titans: Learning to Memorize at Test Time](https://arxiv.org/html/2501.00663v1) 
[WorldGPT: Empowering LLM as Multimodal World Model](https://arxiv.org/html/2404.18202v2) -->





        z_out = self.output_proj(z_out)  # (B,S,vocab_size)
        return z_out

    def get_classifier_weights(self) -> torch.Tensor:
        """
        How:
            cut-cross-entropyライブラリのために、分類器の重みを取得する関数。
            linear_cross_entropyの2番目の引数として使用される。
            
        Returns:
            出力プロジェクションレイヤーの重み (2D,V)
        """
        return self.output_proj.weight