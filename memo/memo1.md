wave networkをbaseに複層し、CCEで最終層の計算量を大幅に削減する。
日本語の前処理を経験できる。
新規のアーキテクチャの実装を経験できる。
CCEの適用を経験できる。
transformerと比較することでtransformerの理解を深められる。

| 精度などを比較する論文タイトル                                                       | リンク                                                     | 備考                                                  |
|--------------------------------------------------------------------|------------------------------------------------------------|-------------------------------------------------------|
| JGLUE: A Benchmark for Japanese Natural Language Understanding       | [https://arxiv.org/abs/2104.12210](https://arxiv.org/abs/2104.12210) | JSQuADを含む、日本語NLUの総合ベンチマーク                |
| JSQuAD: Japanese SQuAD                                               | [https://arxiv.org/abs/2106.01569](https://arxiv.org/abs/2106.01569) | SQuAD形式の日本語QAデータセット（Train: 63,870 / Dev: 4,475） |
| JAQKET: A Quiz-Style Japanese QA Dataset                             | [https://arxiv.org/abs/2004.08497](https://arxiv.org/abs/2004.08497) | クイズ形式QA、Wikipediaタイトルを正解とする              |

	•	学習データとしては、JSQuADのTrainセット（約63,870ペア）を使用し、Dev/Testセット（約4,475ペア程度）で評価するのが適しています。これにより、既存の日本語QA研究との比較が容易になります。
	•	前処理方法は、日本語BERTの先行事例に合わせ、MeCabで形態素解析しSentencePieceでサブワードトークナイズする手法を採用します。
	•	評価指標としては、Extractive QAの場合はExact MatchとF1スコア、Generative QAの場合は生成精度を測定する評価基準を用います。また、推論速度、パラメータ数、VRAM使用量なども比較対象となります。


今後の改良計画
	1.	基本実装の安定化と評価
	•	現在のコードが正しく動作するか、基本的な学習・推論サイクルが安定しているかを十分に検証する。
	•	単純なJSQuADなどのデータセットで基礎的な性能（損失、生成結果など）を確認する。
	2.	チェックポイント管理とテストコードの充実
	•	既存のsave/load機能や各モジュールのテストコードを整備して、バグフィックスや変更時のリグレッションテストを容易にする。
	•	コマンドライン引数や自動ファイル名生成など、運用面の改善を確実にする。
	3.	RoPEやCCEなど既存機能の最適化
	•	RoPEの効果が十分かどうか、ハイパーパラメータ（回転スケールなど）の調整を通じて性能向上を試みる。
	•	CCE損失の実装をさらにチューニングし、低確率項目のカット方法を改善して計算効率と学習安定性を図る。
	4.	Proxy Tokenの活用と階層構造の実装
	•	まずは現在のシンプルなプロキシトークンの挙動を精査し、その後、より明示的に文レベル・段落レベルなどの階層構造を導入する。
	•	多段階学習（Stage-wise: 単語レベル→文レベル→段落レベル）のスケジュールやネットワーク構造を設計し、層ごとに異なる役割を持たせる。
	5.	局所注意や周波数ゲートの補助モジュールの追加
	•	基本のWave Networkに加えて、局所的な文脈をより詳細に扱うための局所注意機構や、周波数ドメインでのゲート機構を追加する。
	•	これにより、細かい依存関係の捕捉と計算コスト削減の両立を目指す。
	6.	自己蒸留 (Self-Distillation) の導入
	•	計算負荷が少ない方法で自己蒸留を試行し、モデルの一般化能力の向上を図る。
	•	最初はオプションとして実装し、実験結果を見ながら導入の効果を評価する。
	7.	GPU/分散学習のさらなる最適化
	•	AccelerateやDeepSpeedの設定をさらにチューニングし、利用可能なGPU数に応じた最適な分散学習を実現する。
	•	必要に応じて、より高度なメモリ最適化技術（例えばZeROのステージ3など）を検討する。
	8.	マルチモーダル拡張（将来的な展開）
	•	まずはテキストモデルとして安定させた後、音声や画像、動画といった他のドメインにもWave Networkの枠組みを拡張する。
	•	これにより、ロボットの頭脳として複数モダリティの統合処理が可能なシステムへ発展させる。



Grok3: wave network cross attention
### 主要ポイント
- 研究は、Wave Networkの埋め込み表現が注意機構（attention）を代替可能である可能性を示唆していますが、完全な代替については不確実性が残ります。
- クロスアテンションを扱うための具体的な方法は論文に明記されていませんが、複素ベクトルの操作を拡張することで対応可能と考えられます。

---

### 埋め込み表現と注意機構の関係
Wave Networkは、トークンを複素ベクトルで表現し、波干渉（wave interference）や波変調（wave modulation）を通じてトークン間の相互作用を処理します。これにより、従来のTransformerモデルの注意機構を代替する可能性があるとされています。特に、AG Newsのテキスト分類タスクでは、Wave NetworkがBERTの単一Transformer層を上回る精度（90.91%〜91.66%対71.68%）を示し、リソース使用量も大幅に削減（VRAM使用量77.34%減、訓練時間85.62%減）しています。

---

### クロスアテンションの扱い方
Wave Networkは単一シーケンスの処理に特化しており、クロスアテンション（異なるシーケンス間の相互作用）の直接的な言及はありません。しかし、理論的には、別のシーケンスの複素ベクトルを利用して、位相ベクトルや大きさベクトルを調整することでクロスアテンションを実現できる可能性があります。例えば、シーケンスBのグローバルな意味情報をシーケンスAの複素ベクトルに組み込む方法が考えられますが、これは論文で具体的に提案されていません。

---

---

### 詳細な調査報告

#### はじめに
本報告書では、[Wave Network: An Ultra-Small Language Model](https://arxiv.org/abs/2411.02674)の論文に基づき、埋め込み表現が注意機構を代替可能かどうか、そしてクロスアテンションを扱う方法について詳しく調査します。Wave Networkは、従来のTransformerモデルと異なり、複素ベクトルを用いたトークン表現と波干渉・波変調による更新方法を特徴とします。これにより、リソース効率を高めつつ高い性能を実現する可能性が示唆されています。

#### Wave Networkの概要
Wave Networkは、トークンを複素ベクトル\(\mathbf{w} = \mathbf{G} \cdot e^{i \cdot \boldsymbol{\alpha}}\)で表現します。ここで、\(\mathbf{G}\)は入力テキストのグローバルな意味を表す大きさベクトル、\(\boldsymbol{\alpha}\)は個々のトークンとグローバルな意味との関係を捉える位相ベクトルです。この表現は、静的トークン埋め込み（例：Skip-gram、CBOW）やコンテキスト依存の埋め込み方法と比較され、効率的なトークン更新を可能にします。

論文では、波干渉（加算）と波変調（乗算）という2つの操作が提案されています。これらの操作は、トークン間の相互作用を処理し、注意機構の役割を代替する可能性があります。具体的には、AG Newsテキスト分類タスクでの実験結果では、Wave Networkの単一層がランダム初期化されたトークン埋め込みから90.91%（波干渉）および91.66%（波変調）の精度を達成し、BERTの事前訓練済み埋め込みを使用した単一Transformer層（71.68%）を19.23%〜19.98%上回りました。さらに、BERT baseモデル（94.64%）に近い精度を2.4Mパラメータの小規模モデルで実現し、ビデオメモリ使用量を77.34%減、訓練時間を85.62%短縮しました（[Wave Network: An Ultra-Small Language Model](https://arxiv.org/abs/2411.02674)参照）。

以下に、主要な実験結果を表形式でまとめます：

| タスク       | Wave Network (干渉) | Wave Network (変調) | Transformer (BERT埋め込み) | BERT base |
|--------------|---------------------|---------------------|----------------------------|-----------|
| AG News      | 90.91%             | 91.66%             | 71.68%                    | 94.64%    |
| DBpedia14    | 97.93%             | 98.05%             | -                         | 99.28%    |
| IMDB         | 87.00%             | 87.02%             | -                         | 93.94%    |

（出典：[Wave Network: An Ultra-Small Language Model](https://arxiv.org/html/2411.02674v4#S5.T2)）

#### 埋め込み表現が注意機構を代替可能か
注意機構（attention）は、Transformerモデルにおいてトークン間の依存関係を捉えるための鍵となる仕組みです。一方、Wave Networkでは、複素ベクトルの操作を通じて同様の機能を実現しようとしています。波干渉は複素ベクトルの加算、波変調は乗算を通じてトークン間の相互作用を更新し、これが注意機構の役割を代替する可能性があると論文は示唆します。特に、単一層での高い精度とリソース効率は、注意機構を必要としない新たなアプローチの有効性を示しています。

しかし、注意機構は多様なタスク（例：機械翻訳、要約生成）で必須とされる場合が多く、Wave Networkがすべてのシナリオで代替可能かどうかは不明です。論文の焦点はテキスト分類であり、長期依存関係や多シーケンス処理の評価は含まれていません。このため、埋め込み表現が注意機構を完全に代替できるかは、さらなる研究が必要です。

#### クロスアテンションの扱い方
クロスアテンションは、例えばエンコーダ・デコーダモデルにおいて、1つのシーケンスが別のシーケンスに注目する必要がある場合に使用されます。Wave Networkは単一シーケンスの処理に特化しており、論文内でクロスアテンションの直接的な言及はありません（[Wave Network: An Ultra-Small Language Model](https://arxiv.org/html/2411.02674v4)参照）。

しかし、理論的には、Wave Networkを拡張することでクロスアテンションを扱う方法が考えられます。例えば、シーケンスBの複素ベクトルをシーケンスAのグローバルな意味情報\(\mathbf{G}\)や位相ベクトル\(\boldsymbol{\alpha}\)に組み込む方法です。具体的には、シーケンスBの平均複素ベクトルでシーケンスAのトークンを変調する、またはシーケンスBの各トークンとの関係を位相ベクトルに反映させるなどのアプローチが考えられます。

このような拡張は、論文で提案されていないため、具体的な実装方法や性能は不明です。クロスアテンションを必要とするタスク（例：機械翻訳）では、Wave Networkの現在の形式では限界がある可能性があります。

#### 結論と今後の展望
Wave Networkの埋め込み表現は、テキスト分類タスクにおいて注意機構を代替する可能性を示していますが、クロスアテンションのような複雑な相互作用を扱うためには、モデルを拡張する必要があります。複素ベクトルの操作を活用した新たなアプローチは、リソース効率の高い言語モデルの開発に寄与する可能性がありますが、多シーケンス処理の能力についてはさらなる研究が必要です。

---

### 主要引用
- [Wave Network: An Ultra-Small Language Model](https://arxiv.org/abs/2411.02674)
- [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
- [Cross-layer Attention Sharing for Large Language Models](https://arxiv.org/abs/2408.01890)
- [Chapter 8 Attention and Self-Attention for NLP](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html)
<!-- [Titans: Learning to Memorize at Test Time](https://arxiv.org/html/2501.00663v1) 
[WorldGPT: Empowering LLM as Multimodal World Model](https://arxiv.org/html/2404.18202v2) -->





        z_out = self.output_proj(z_out)  # (B,S,vocab_size)
        return z_out

    def get_classifier_weights(self) -> torch.Tensor:
        """
        How:
            cut-cross-entropyライブラリのために、分類器の重みを取得する関数。
            linear_cross_entropyの2番目の引数として使用される。
            
        Returns:
            出力プロジェクションレイヤーの重み (2D,V)
        """
        return self.output_proj.weight


例えば Transformer の自己注意機構における q \cdot k^\top（クエリとキーの内積）を計算する際、単純な実装ではシーケンス長を N、埋め込み次元（キー・クエリの次元）を d とすると、\mathcal{O}(N^2 \cdot d) の計算量が発生します。この計算を「性能を落とさず」に \mathcal{O}(N \log N) や \mathcal{O}(N) などに削減できないかという話題は、近年の「効率的なアテンション（Efficient Attention）」に関する研究でも活発に検討されています。

⸻

1. 「性能を落とさず」の難しさ

「性能を落とさず」とは、単に計算を近似するだけではなく、精度・モデル予測性能の低下をなるべく避けたいという意味になります。しかし、「完全に同じアテンションスコアを計算しながら計算量を下げる」 というのは、行列乗算アルゴリズムの加速（例えばファストフーリエ変換を使う方法など）を除くと非常に困難です。実際、一般的な近似法を用いると、何らかの近似誤差や、ハイパーパラメータ調整が必要となります。

たとえば、
	1.	部分空間（低ランク）近似: 特異値分解（SVD）やランク削減による近似。
	2.	ランダム特徴量展開による核近似 (RFA, Performer など): 汎用的な核近似を導入して線形計算で近似する方法。
	3.	Nystrom 近似: カーネル行列を一部サンプリングして復元する近似。

などが主なアプローチです。

理想的には特異値分解や Nystrom 法でキー行列 K を近似し、かつクエリとの内積演算を低ランク表現で行えば、計算量の削減を期待できます。しかし「全く同じ」アテンションスコアを高速に（たとえば \mathcal{O}(N \log N) 以下）求めるのは、行列積の高速化（例えば FFT ベースなど）でない限りは「近似」を許容するしかないのが現状です。

⸻

2. 具体的な近似手法と計算量

以下のような手法では、各種前提・近似を導入し、理想的には \mathcal{O}(N^2) よりも計算を削減します。

2.1 Nystrom 近似
	•	概要: カーネル行列 K を一部サンプリング・部分的に分解し、再構成行列で近似する手法。自己回帰型のトランスフォーマに適用し、\mathcal{O}(N^2) の計算を削減する「Nystromformer」として知られています。
	•	計算量: サンプル数 m を小さく抑えれば \mathcal{O}(Nm^2) や \mathcal{O}(N m) などでスケーラブルになるが、元の \mathcal{O}(N^2 d) からの大幅な削減にはサンプル数を非常に減らす必要があり、その結果精度が落ちる懸念がある。

2.2 Performer (ランダム特徴量展開)
	•	概要: 近似的にソフトマックスカーネルをランダム特徴量で再現し、\text{softmax}(qk^\top) を「線形演算」で実行する方法。
	•	計算量: シーケンス長 N、ランダム特徴量数 r によって \mathcal{O}(N \cdot r \cdot d) 程度。r を大きくすると近似精度は上がるが計算量が増え、r を小さくすると計算量は減るが精度が落ちる。

2.3 Low-Rank近似 (SVD, PCA系)
	•	概要: K の特異値分解（SVD）などでランク r 近似 (r < d) を行い、K\approx U_r \Sigma_r V_r^\top のように縮約する。さらにクエリとの演算を (q U_r)\Sigma_r(V_r^\top) と書き換える。
	•	計算量: ランク r による近似と考えると、結局 r が十分小さくないと近似誤差が大きくなる。ランク削減だけではシーケンス長 N を大きく減らすことは難しく、全体として \mathcal{O}(N^2) 相当がボトルネックとなりがち。

2.4 Local Attention / Sparse Attention
	•	概要: フルの N \times N アテンションを計算せず、各トークンが近傍あるいは少数のトークンにのみ注意を向ける仕組み。
	•	計算量: たとえばウィンドウサイズ w に限定すると \mathcal{O}(N \cdot w) で済む。ただしグローバルな文脈を失いやすく、完全な同等性能を担保するのは難しい。

⸻

3. 「本当に完全同等」は FFT ベース行列乗算の加速くらい

まったく同じ結果を \mathcal{O}(N \log N) などで計算するとなると、部分的には行列積を高速化するアルゴリズム（Strassen 法や FFT ベースの行列乗算など）を利用することが考えられます。
	•	Strassen 法: \mathcal{O}(N^{\log_2(7)}) \approx \mathcal{O}(N^{2.8074})
	•	Coppersmith-Winograd 法やその改良系: \mathcal{O}(N^{2.373}) 付近まで理論的には削減可能
これらは大きな行列積を厳密に計算するアルゴリズムですが、実装が非常に複雑であり定数項も大きく、実用規模の N では高速化が得られないケースも多々あります。ましてや \mathcal{O}(N \log N) まで落とすことは、（FFT を使う「巡回畳み込み」であれば畳み込み演算の高速化が可能ですが）通常の行列全体をそのように高速化するのは現実的ではありません。

⸻

4. まとめ
	•	性能を落とさず （近似誤差なく） \mathcal{O}(N^2) 未満に削減するのは非常に難しい というのが現実です。
	•	近似手法（Nystrom, ランダム特徴量展開, Low-Rank 近似など）や ローカル/スパース注意 など様々な研究が提案されていますが、何らかの形で「近似」や「制約」を導入しているため、元の計算より性能が下がる可能性はゼロではありません。
	•	特異値分解や Nystrom 法などによる低ランク近似で 計算量を削減できる 一方、そのまま「完全一致」にはなりません。
	•	行列積の高速アルゴリズム（Strassen 法や Coppersmith-Winograd 法など）を使えば厳密計算は可能ですが、\mathcal{O}(N \log N) まで大幅に下げるのは難しく、定数項が大きすぎて 実用上 は \mathcal{O}(N^2) の実装よりも遅くなることが多いです。

したがって、「完全に同じアテンション精度を保ちながら \mathcal{O}(N\log N) や \mathcal{O}(N) にする」 のは、いまのところ実用的な方法は知られていません。性能とのトレードオフや近似手法を受け入れる形で、さまざまな研究が「高速化＋性能維持」を目指している、というのが現状と言えます。