#!/usr/bin/env python
# coding: utf-8
# tests/test_tokenizer_special_tokens.py
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ

import os
import sys
import argparse

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from slm.tokenizer import load_tokenizer

def test_tokenizer_functions(tokenizer):
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åŸºæœ¬æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")
    
    test_text = "ã“ã‚Œã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãŒæ­£ã—ãæ©Ÿèƒ½ã™ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚"
    print(f"ãƒ†ã‚¹ãƒˆæ–‡: {test_text}")
    
    # åŸºæœ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰
    token_ids = tokenizer.encode(test_text, add_special_tokens=False)
    decoded_text = tokenizer.decode(token_ids)
    
    print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {token_ids}")
    
    if hasattr(tokenizer, 'convert_ids_to_tokens'):
        tokens = tokenizer.convert_ids_to_tokens(token_ids)
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ä¸€è¦§: {tokens}")
    
    print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: {decoded_text}")
    print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰â†’ãƒ‡ã‚³ãƒ¼ãƒ‰ä¸€è‡´: {'ã¯ã„' if test_text == decoded_text else 'ã„ã„ãˆ'}")
    
    return token_ids

def test_special_tokens(tokenizer):
    """ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")
    
    # åˆ©ç”¨å¯èƒ½ãªç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºèª
    special_tokens = {
        "<mask>": getattr(tokenizer, "mask_token", None),
        "<pad>": getattr(tokenizer, "pad_token", None),
        "<bos>": getattr(tokenizer, "bos_token", None),
        "<eos>": getattr(tokenizer, "eos_token", None),
        "<unk>": getattr(tokenizer, "unk_token", None),
        "<cls>": getattr(tokenizer, "cls_token", None),
        "<sep>": getattr(tokenizer, "sep_token", None),
    }
    
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã®è¡¨ç¤º
    print("ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä¸€è¦§:")
    for name, token in special_tokens.items():
        if token:
            token_id = None
            if name == "<mask>" and hasattr(tokenizer, "mask_token_id"):
                token_id = tokenizer.mask_token_id
            elif name == "<pad>" and hasattr(tokenizer, "pad_token_id"):
                token_id = tokenizer.pad_token_id
            elif name == "<bos>" and hasattr(tokenizer, "bos_token_id"):
                token_id = tokenizer.bos_token_id
            elif name == "<eos>" and hasattr(tokenizer, "eos_token_id"):
                token_id = tokenizer.eos_token_id
            elif name == "<unk>" and hasattr(tokenizer, "unk_token_id"):
                token_id = tokenizer.unk_token_id
            elif name == "<cls>" and hasattr(tokenizer, "cls_token_id"):
                token_id = tokenizer.cls_token_id
            elif name == "<sep>" and hasattr(tokenizer, "sep_token_id"):
                token_id = tokenizer.sep_token_id
                
            print(f"  {name}: '{token}' (ID: {token_id})")
    
    # èªå½™ã‚µã‚¤ã‚ºã®ç¢ºèª
    if hasattr(tokenizer, "vocab_size"):
        print(f"\nèªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size}")
    
    # å…¨ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã®ç¢ºèª
    if hasattr(tokenizer, "all_special_tokens"):
        print(f"å…¨ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³: {tokenizer.all_special_tokens}")
        print(f"å…¨ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ID: {tokenizer.all_special_ids}")
    
    # <mask>ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ†ã‚¹ãƒˆ
    if getattr(tokenizer, "mask_token", None):
        test_mask_token(tokenizer)
    
    # <pad>ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ†ã‚¹ãƒˆ
    if getattr(tokenizer, "pad_token", None):
        test_pad_token(tokenizer)
    
    # <unk>ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ†ã‚¹ãƒˆ
    if getattr(tokenizer, "unk_token", None):
        test_unk_token(tokenizer)

def test_mask_token(tokenizer):
    """<mask>ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°ãƒ†ã‚¹ãƒˆ"""
    print("\n--- <mask>ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ ---")
    
    # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    mask_token = tokenizer.mask_token
    mask_token_id = tokenizer.mask_token_id
    print(f"<mask>ãƒˆãƒ¼ã‚¯ãƒ³: '{mask_token}' (ID: {mask_token_id})")
    
    # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    test_text = f"ã“ã‚Œã¯{mask_token}ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚"
    token_ids = tokenizer.encode(test_text, add_special_tokens=False)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±è¡¨ç¤º
    if hasattr(tokenizer, 'convert_ids_to_tokens'):
        tokens = tokenizer.convert_ids_to_tokens(token_ids)
        print(f"ãƒã‚¹ã‚¯ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {token_ids}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ä¸€è¦§: {tokens}")
    
    # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’ç¢ºèª
    mask_positions = [i for i, id in enumerate(token_ids) if id == mask_token_id]
    print(f"ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®: {mask_positions}")
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœç¢ºèª
    decoded_text = tokenizer.decode(token_ids)
    print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: {decoded_text}")
    print(f"ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹: {'ã¯ã„' if mask_token in decoded_text else 'ã„ã„ãˆ'}")
    
    # æ¨™æº–çš„ãªæ–‡ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ç‰¹å®šã®ä½ç½®ã‚’ãƒã‚¹ã‚¯ã«ç½®ãæ›ãˆ
    normal_text = "ã“ã‚Œã¯ãƒã‚¹ã‚¯ç½®æ›ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚"
    normal_ids = tokenizer.encode(normal_text, add_special_tokens=False)
    
    # "ãƒã‚¹ã‚¯"ã®ä½ç½®ã‚’ç‰¹å®š (å®Ÿéš›ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ç•°ãªã‚‹å¯èƒ½æ€§ã‚ã‚Š)
    if hasattr(tokenizer, 'convert_ids_to_tokens'):
        tokens = tokenizer.convert_ids_to_tokens(normal_ids)
        print(f"\nå…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {normal_text}")
        print(f"å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")
    
    # "ãƒã‚¹ã‚¯"æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã‚‹éƒ¨åˆ†ã‚’æ¢ã™
    mask_target_idx = 2  # è¿‘ä¼¼ä½ç½®ã¨ã—ã¦ä»®å®š
    
    # ãƒã‚¹ã‚¯ç½®æ›
    masked_ids = normal_ids.copy()
    masked_ids[mask_target_idx] = mask_token_id
    
    # ç½®æ›çµæœç¢ºèª
    if hasattr(tokenizer, 'convert_ids_to_tokens'):
        masked_tokens = tokenizer.convert_ids_to_tokens(masked_ids)
        print(f"ãƒã‚¹ã‚¯ç½®æ›å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³: {masked_tokens}")
    
    masked_text = tokenizer.decode(masked_ids)
    print(f"ãƒã‚¹ã‚¯ç½®æ›å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ: {masked_text}")

def test_pad_token(tokenizer):
    """<pad>ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°ãƒ†ã‚¹ãƒˆ"""
    print("\n--- <pad>ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ ---")
    
    # ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    pad_token = tokenizer.pad_token
    pad_token_id = tokenizer.pad_token_id
    print(f"<pad>ãƒˆãƒ¼ã‚¯ãƒ³: '{pad_token}' (ID: {pad_token_id})")
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    test_text = "ã“ã‚Œã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚"
    token_ids = tokenizer.encode(test_text, add_special_tokens=False)
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ 
    padded_ids = token_ids + [pad_token_id] * 3
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±è¡¨ç¤º
    if hasattr(tokenizer, 'convert_ids_to_tokens'):
        tokens = tokenizer.convert_ids_to_tokens(padded_ids)
        print(f"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‰ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}")
        print(f"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œãƒˆãƒ¼ã‚¯ãƒ³ID: {padded_ids}")
        print(f"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œãƒˆãƒ¼ã‚¯ãƒ³ä¸€è¦§: {tokens}")
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœç¢ºèª
    decoded_text = tokenizer.decode(padded_ids)
    print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: {decoded_text}")
    
    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ãŸå ´åˆã®æŒ™å‹•ï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰
    attention_mask = [1] * len(token_ids) + [0] * 3
    print(f"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯: {attention_mask}")
    print("æ³¨: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãŒ0ã®ä½ç½®ã®ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—ã§ç„¡è¦–ã•ã‚Œã¾ã™")

def test_unk_token(tokenizer):
    """<unk>ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°ãƒ†ã‚¹ãƒˆ"""
    print("\n--- <unk>ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ ---")
    
    # UNKãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    unk_token = tokenizer.unk_token
    unk_token_id = tokenizer.unk_token_id
    print(f"<unk>ãƒˆãƒ¼ã‚¯ãƒ³: '{unk_token}' (ID: {unk_token_id})")
    
    # èªå½™å¤–ã®æ–‡å­—/å˜èªã‚’ãƒ†ã‚¹ãƒˆ
    oov_chars = "ğŸ˜ŠğŸš€ğŸ‘"  # çµµæ–‡å­—ãªã©ã®ç‰¹æ®Šæ–‡å­—
    token_ids = tokenizer.encode(oov_chars, add_special_tokens=False)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±è¡¨ç¤º
    if hasattr(tokenizer, 'convert_ids_to_tokens'):
        tokens = tokenizer.convert_ids_to_tokens(token_ids)
        print(f"èªå½™å¤–æ–‡å­—: {oov_chars}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID: {token_ids}")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ä¸€è¦§: {tokens}")
    
    # UNKãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºç¾å›æ•°ç¢ºèª
    unk_count = token_ids.count(unk_token_id) if unk_token_id is not None else 0
    print(f"UNKãƒˆãƒ¼ã‚¯ãƒ³å‡ºç¾å›æ•°: {unk_count}")
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœç¢ºèª
    decoded_text = tokenizer.decode(token_ids)
    print(f"ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: {decoded_text}")
    print(f"å…ƒã®æ–‡å­—åˆ—ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹: {'ã¯ã„' if oov_chars == decoded_text else 'ã„ã„ãˆ'}")

def parse_args():
    parser = argparse.ArgumentParser(description="ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ")
    
    parser.add_argument("--tokenizer_name", type=str, default="megagonlabs/t5-base-japanese-web",
                       help="ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å")
    
    return parser.parse_args()

def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    args = parse_args()
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ '{args.tokenizer_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    tokenizer = load_tokenizer(args.tokenizer_name)
    print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {tokenizer.__class__.__name__}")
    
    # åŸºæœ¬çš„ãªæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
    test_tokenizer_functions(tokenizer)
    
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚¹ãƒˆ
    test_special_tokens(tokenizer)
    
    print("\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()