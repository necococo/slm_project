{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM: Wiki40B日本語データセットでのDiffusionモデル学習\n",
    "\n",
    "このノートブックでは、日本語Wiki40Bデータセットを使用してWave Network言語モデルをDiffusionアプローチで学習します。\n",
    "\n",
    "## 1. 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Google Driveをマウント\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# プロジェクトのクローンとインストール\n",
    "!git clone https://github.com/yourusername/slm_project.git\n",
    "!cd slm_project && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install sentencepiece datasets transformers==4.31.0 accelerate==0.21.0 pywavelets==1.4.1 bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wiki40B日本語データセットの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from slm.wiki40b_ja_dataset import prepare_dataset, train_tokenizer\n",
    "\n",
    "# Google Drive上のデータディレクトリ\n",
    "data_dir = \"/content/drive/MyDrive/slm/data/wiki40b_ja\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# データセットをダウンロードして前処理\n",
    "train_path, valid_path, test_path = prepare_dataset(data_dir)\n",
    "print(f\"データセット前処理完了。保存パス: {train_path}, {valid_path}, {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# SentencePieceトークナイザーの学習\n",
    "model_prefix = \"sp_jwiki\"\n",
    "vocab_size = 32000\n",
    "\n",
    "train_tokenizer(train_path, valid_path, data_dir, model_prefix, vocab_size)\n",
    "print(f\"トークナイザーの学習完了。モデルファイル: {os.path.join(data_dir, model_prefix)}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# トークナイザーのテスト\n",
    "from slm.wiki40b_ja_dataset import load_tokenizer, test_tokenizer_functionality\n",
    "\n",
    "tokenizer = load_tokenizer(data_dir, model_prefix)\n",
    "test_tokenizer_functionality(tokenizer, \"これはトークナイザーのテストです。日本語Wikipediaで学習されたモデルを使います。\")\n",
    "\n",
    "# [MASK]トークンのIDを確認\n",
    "mask_id = tokenizer.piece_to_id(\"[MASK]\")\n",
    "print(f\"[MASK]トークンID: {mask_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Diffusionモデル学習の実行\n",
    "!cd /content/slm_project && python slm/train_wiki40b_ja_diffusion.py \\\n",
    "    --data_dir=\"/content/drive/MyDrive/slm/data/wiki40b_ja\" \\\n",
    "    --output_dir=\"/content/drive/MyDrive/slm/outputs\" \\\n",
    "    --model_prefix=\"sp_jwiki\" \\\n",
    "    --hidden_size=1024 \\\n",
    "    --num_layers=3 \\\n",
    "    --max_seq_len=512 \\\n",
    "    --batch_size=8 \\\n",
    "    --epochs=3 \\\n",
    "    --learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from slm.modules.wave_network import WaveNetworkLM\n",
    "from slm.config import ModelConfig\n",
    "from slm.wiki40b_ja_dataset import load_tokenizer\n",
    "\n",
    "# モデルチェックポイントパス\n",
    "output_dir = \"/content/drive/MyDrive/slm/outputs\"\n",
    "run_dirs = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d)) and d.startswith(\"wiki40b_ja_diffusion\")]\n",
    "run_dirs.sort()\n",
    "latest_run = run_dirs[-1] if run_dirs else None\n",
    "\n",
    "if latest_run:\n",
    "    checkpoint_dir = os.path.join(output_dir, latest_run, \"checkpoints\")\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pt\")]\n",
    "    checkpoint_files.sort()\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1]) if checkpoint_files else None\n",
    "    \n",
    "    print(f\"最新の実行: {latest_run}\")\n",
    "    print(f\"利用可能なチェックポイント: {checkpoint_files}\")\n",
    "    print(f\"最新のチェックポイント: {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"学習済みのモデルが見つかりません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 学習済みモデルのロード（上記で最新のチェックポイントが見つかった場合）\n",
    "if 'latest_checkpoint' in locals() and latest_checkpoint:\n",
    "    # トークナイザーのロード\n",
    "    data_dir = \"/content/drive/MyDrive/slm/data/wiki40b_ja\"\n",
    "    model_prefix = \"sp_jwiki\"\n",
    "    tokenizer = load_tokenizer(data_dir, model_prefix)\n",
    "    \n",
    "    # チェックポイントのロード\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location='cpu')\n",
    "    model_config = checkpoint[\"model_config\"]\n",
    "    \n",
    "    # トークナイザーをモデルに設定\n",
    "    model_config.set_tokenizer(tokenizer)\n",
    "    \n",
    "    # モデルのインスタンス化と重みのロード\n",
    "    model = WaveNetworkLM(model_config)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    print(f\"モデルを正常にロードしました。パラメータ数: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "else:\n",
    "    print(\"学習済みモデルがロードできませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 簡単な推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 簡単な推論テスト（モデルがロードされている場合）\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    from slm.diffusion import SimpleTextDiffusion\n",
    "    import torch\n",
    "    \n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # マスクトークンIDの取得\n",
    "    mask_token_id = tokenizer.piece_to_id(\"[MASK]\")\n",
    "    \n",
    "    # Diffusionモデルのインスタンス化\n",
    "    diffuser = SimpleTextDiffusion(\n",
    "        timesteps=20,\n",
    "        mask_token_id=mask_token_id,\n",
    "        vocab_size=tokenizer.get_piece_size()\n",
    "    ).to(device)\n",
    "    \n",
    "    # テスト文の用意\n",
    "    test_text = \"日本は四季折々の自然が美しい国です。\"\n",
    "    print(f\"元のテキスト: {test_text}\")\n",
    "    \n",
    "    # トークン化\n",
    "    tokens = tokenizer.encode(test_text, out_type=int)\n",
    "    token_tensor = torch.tensor([tokens], device=device)\n",
    "    \n",
    "    # 完全にノイズを加える（最大タイムステップ)\n",
    "    t = torch.tensor([diffuser.timesteps - 1], device=device)\n",
    "    noisy_tokens, _ = diffuser(token_tensor, t)\n",
    "    \n",
    "    # ノイズを加えたテキストの表示\n",
    "    noisy_text = tokenizer.decode(noisy_tokens[0].cpu().tolist())\n",
    "    print(f\"ノイズを加えたテキスト: {noisy_text}\")\n",
    "    \n",
    "    # 逐次的にノイズを除去する簡易版デノイズプロセス\n",
    "    with torch.no_grad():\n",
    "        current_tokens = noisy_tokens.clone()\n",
    "        \n",
    "        for timestep in reversed(range(diffuser.timesteps)):\n",
    "            # マスクされた位置を見つける\n",
    "            mask_positions = (current_tokens == mask_token_id)\n",
    "            \n",
    "            if not mask_positions.any():\n",
    "                break\n",
    "                \n",
    "            # モデルの予測を取得\n",
    "            logits = model(current_tokens)\n",
    "            \n",
    "            # マスクされた位置でのトップk予測を取得\n",
    "            k = 5\n",
    "            topk_probs, topk_indices = torch.topk(logits.softmax(dim=-1), k, dim=-1)\n",
    "            \n",
    "            # マスクごとにトップ1の予測で置き換え\n",
    "            for i in range(current_tokens.size(0)):\n",
    "                for j in range(current_tokens.size(1)):\n",
    "                    if mask_positions[i, j]:\n",
    "                        # ここではトップ1の予測を使用\n",
    "                        current_tokens[i, j] = topk_indices[i, j, 0]\n",
    "                        \n",
    "            # 結果を表示\n",
    "            if timestep % 5 == 0 or timestep == 0:\n",
    "                current_text = tokenizer.decode(current_tokens[0].cpu().tolist())\n",
    "                print(f\"タイムステップ {timestep} での復元: {current_text}\")\n",
    "        \n",
    "        final_text = tokenizer.decode(current_tokens[0].cpu().tolist())\n",
    "        print(f\"\\n最終的な復元テキスト: {final_text}\")\n",
    "else:\n",
    "    print(\"モデルがロードされていないため、推論テストを実行できません\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}