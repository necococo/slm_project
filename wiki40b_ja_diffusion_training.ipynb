{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM: Wiki40B日本語データセットでのDiffusionモデル学習\n",
    "\n",
    "このノートブックでは、日本語Wiki40Bデータセットを使用してWave Network言語モデルをDiffusionアプローチで学習します。\n",
    "\n",
    "## 1. 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Google Driveをマウント\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# プロジェクトのクローンとインストール\n",
    "!git clone https://github.com/yourusername/slm_project.git\n",
    "!cd slm_project && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install sentencepiece datasets transformers==4.31.0 accelerate==0.21.0 pywavelets==1.4.1 bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wiki40B日本語データセットの前処理"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 必要なライブラリをインポート\nimport os\nfrom datasets import load_dataset\n\n# Google Drive上のデータディレクトリ\ndata_dir = \"/content/drive/MyDrive/slm/data/wiki40b_ja\"\nos.makedirs(data_dir, exist_ok=True)\n\n# 最新バージョンのslmパッケージを使用するために更新\n!cd /content/slm_project && pip install -e .\n\n# toramaru-u/wiki40b-ja データセットを直接ロード (前処理済み)\ndataset = load_dataset(\"toramaru-u/wiki40b-ja\")\n\n# データセット情報を表示\nprint(f\"データセット構造:\")\nprint(f\"  スプリット: {list(dataset.keys())}\")\nfor split in dataset:\n    print(f\"  {split}: {len(dataset[split])}サンプル\")\n\n# ローカルに保存（将来の使用のため）\ndataset.save_to_disk(data_dir)\nprint(f\"データセット保存完了。保存パス: {data_dir}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# megagonlabs/t5-base-japanese-webトークナイザーをロード\nfrom transformers import AutoTokenizer\nfrom slm.tokenizer import JapaneseTokenizer\n\ntokenizer_name = \"megagonlabs/t5-base-japanese-web\"\nprint(f\"トークナイザー {tokenizer_name} をロード中...\")\n\n# transformersからHuggingFaceトークナイザーをロード\nhf_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n# JapaneseTokenizerラッパーに変換\njp_tokenizer = JapaneseTokenizer.from_pretrained_tokenizer(hf_tokenizer)\n\nprint(f\"トークナイザーをロードしました。語彙サイズ: {len(hf_tokenizer.vocab) if hasattr(hf_tokenizer, 'vocab') else hf_tokenizer.vocab_size}\")\n\n# トークナイザーを保存（必要に応じて）\ntokenizer_path = os.path.join(data_dir, \"tokenizer\")\nos.makedirs(tokenizer_path, exist_ok=True)\nhf_tokenizer.save_pretrained(tokenizer_path)\nprint(f\"トークナイザーを保存しました: {tokenizer_path}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# トークナイザーのテスト\ntest_text = \"これはトークナイザーのテストです。日本語Wikipediaで学習されたモデルを使います。\"\nprint(f\"テスト文: {test_text}\")\n\n# トークン化\ntokens_ids = jp_tokenizer.encode(test_text)\ntokens_str = hf_tokenizer.convert_ids_to_tokens(tokens_ids)\nprint(f\"トークンID: {tokens_ids}\")\nprint(f\"トークン: {tokens_str}\")\n\n# デコード (例外処理対応)\ntry:\n    # 新しいdecodeメソッドを試す\n    decoded_text = jp_tokenizer.decode(tokens_ids, skip_special_tokens=True)\nexcept TypeError:\n    # 引数エラーが出た場合は、通常のデコードを行い、後で特殊トークンを削除\n    decoded_text = jp_tokenizer.decode(tokens_ids)\n    # 特殊トークン（</s>など）を削除\n    decoded_text = decoded_text.replace(\"</s>\", \"\")\n\nprint(f\"デコード結果: {decoded_text}\")\n\n# 一致確認\nif test_text == decoded_text:\n    print(\"✓ 完全一致しました\")\nelse:\n    print(\"× 不一致があります\")\n    print(f\"  元のテキスト: {test_text}\")\n    print(f\"  デコード結果: {decoded_text}\")\n\n# 特殊トークンを含むデコード結果も表示\ntry:\n    decoded_with_special = jp_tokenizer.decode(tokens_ids, skip_special_tokens=False)\nexcept TypeError:\n    decoded_with_special = jp_tokenizer.decode(tokens_ids)\n\nprint(f\"\\n特殊トークンを含むデコード結果: {decoded_with_special}\")\n\n# [MASK]トークンのIDを確認\nmask_token_id = jp_tokenizer.mask_token_id\nprint(f\"[MASK]トークンID: {mask_token_id}\")\n\n# トークナイザー情報\nprint(f\"\\nトークナイザー情報:\")\nprint(f\"  元のトークナイザー: {hf_tokenizer.__class__.__name__}\")\nprint(f\"  語彙サイズ: {len(hf_tokenizer.vocab) if hasattr(hf_tokenizer, 'vocab') else hf_tokenizer.vocab_size}\")\nspecial_tokens = {\n    \"MASK\": hf_tokenizer.mask_token if hasattr(hf_tokenizer, 'mask_token') else None,\n    \"PAD\": hf_tokenizer.pad_token if hasattr(hf_tokenizer, 'pad_token') else None,\n    \"EOS\": hf_tokenizer.eos_token if hasattr(hf_tokenizer, 'eos_token') else None,\n    \"BOS\": hf_tokenizer.bos_token if hasattr(hf_tokenizer, 'bos_token') else None\n}\nfor name, token in special_tokens.items():\n    print(f\"  {name}: {token if token else 'なし'}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# データセットの前処理を行い、トークン化\nfrom datasets import load_from_disk\n\n# すでに保存されたデータセットをロード\ndataset = load_from_disk(data_dir)\n\n# トークン化関数\ndef tokenize_function(examples):\n    tokenized = {\"input_ids\": [], \"attention_mask\": []}\n    \n    for text in examples[\"text\"]:\n        # トークン化\n        token_ids = jp_tokenizer.encode(text)\n        \n        # 最大長に切り詰め (512トークン)\n        if len(token_ids) > 512:\n            token_ids = token_ids[:512]\n        \n        # 注意マスクを作成（すべて1）\n        attn_mask = [1] * len(token_ids)\n        \n        tokenized[\"input_ids\"].append(token_ids)\n        tokenized[\"attention_mask\"].append(attn_mask)\n    \n    return tokenized\n\n# データセットをトークン化\nprint(\"データセットをトークン化中...\")\ntokenized_datasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=1000,\n    remove_columns=[\"text\"]\n)\n\n# 検証セットのサイズを制限（メモリ節約のため）\nif \"validation\" in tokenized_datasets:\n    tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].select(range(min(len(tokenized_datasets[\"validation\"]), 1000)))\n\nprint(\"トークン化済みデータセット情報:\")\nfor split in tokenized_datasets:\n    print(f\"  {split}: {len(tokenized_datasets[split])}サンプル\")\n\n# トークン化済みデータセットを保存\ntokenized_path = os.path.join(data_dir, \"tokenized\")\ntokenized_datasets.save_to_disk(tokenized_path)\nprint(f\"トークン化済みデータセットを保存しました: {tokenized_path}\")\n\n# 新しいDiffusionモデル学習スクリプトを実行\n!cd /content/slm_project && python slm/train_wiki40b_ja_diffusion_megagon.py \\\n    --use_local_dataset \\\n    --local_data_dir=\"{tokenized_path}\" \\\n    --output_dir=\"/content/drive/MyDrive/slm/outputs\" \\\n    --tokenizer_name=\"megagonlabs/t5-base-japanese-web\" \\\n    --hidden_size=1024 \\\n    --num_layers=3 \\\n    --max_seq_len=512 \\\n    --batch_size=8 \\\n    --epochs=3 \\\n    --learning_rate=2e-5",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from slm.modules.wave_network import WaveNetworkLM\n",
    "from slm.config import ModelConfig\n",
    "from slm.wiki40b_ja_dataset import load_tokenizer\n",
    "\n",
    "# モデルチェックポイントパス\n",
    "output_dir = \"/content/drive/MyDrive/slm/outputs\"\n",
    "run_dirs = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d)) and d.startswith(\"wiki40b_ja_diffusion\")]\n",
    "run_dirs.sort()\n",
    "latest_run = run_dirs[-1] if run_dirs else None\n",
    "\n",
    "if latest_run:\n",
    "    checkpoint_dir = os.path.join(output_dir, latest_run, \"checkpoints\")\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pt\")]\n",
    "    checkpoint_files.sort()\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1]) if checkpoint_files else None\n",
    "    \n",
    "    print(f\"最新の実行: {latest_run}\")\n",
    "    print(f\"利用可能なチェックポイント: {checkpoint_files}\")\n",
    "    print(f\"最新のチェックポイント: {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"学習済みのモデルが見つかりません\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 学習済みモデルのロード\nif 'latest_checkpoint' in locals() and latest_checkpoint:\n    from transformers import AutoTokenizer\n    from slm.tokenizer import JapaneseTokenizer\n    \n    # トークナイザーのロード\n    tokenizer_name = \"megagonlabs/t5-base-japanese-web\"\n    hf_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    tokenizer = JapaneseTokenizer.from_pretrained_tokenizer(hf_tokenizer)\n    \n    # チェックポイントのロード\n    checkpoint = torch.load(latest_checkpoint, map_location='cpu')\n    model_config = checkpoint[\"model_config\"]\n    \n    # トークナイザーをモデルに設定\n    model_config.set_tokenizer(tokenizer)\n    \n    # モデルのインスタンス化と重みのロード\n    model = WaveNetworkLM(model_config)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    \n    print(f\"モデルを正常にロードしました。パラメータ数: {sum(p.numel() for p in model.parameters()):,}\")\nelse:\n    print(\"学習済みモデルがロードできませんでした\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 簡単な推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 簡単な推論テスト（モデルがロードされている場合）\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    from slm.diffusion import SimpleTextDiffusion\n",
    "    import torch\n",
    "    \n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # マスクトークンIDの取得\n",
    "    mask_token_id = tokenizer.piece_to_id(\"[MASK]\")\n",
    "    \n",
    "    # Diffusionモデルのインスタンス化\n",
    "    diffuser = SimpleTextDiffusion(\n",
    "        timesteps=20,\n",
    "        mask_token_id=mask_token_id,\n",
    "        vocab_size=tokenizer.get_piece_size()\n",
    "    ).to(device)\n",
    "    \n",
    "    # テスト文の用意\n",
    "    test_text = \"日本は四季折々の自然が美しい国です。\"\n",
    "    print(f\"元のテキスト: {test_text}\")\n",
    "    \n",
    "    # トークン化\n",
    "    tokens = tokenizer.encode(test_text, out_type=int)\n",
    "    token_tensor = torch.tensor([tokens], device=device)\n",
    "    \n",
    "    # 完全にノイズを加える（最大タイムステップ)\n",
    "    t = torch.tensor([diffuser.timesteps - 1], device=device)\n",
    "    noisy_tokens, _ = diffuser(token_tensor, t)\n",
    "    \n",
    "    # ノイズを加えたテキストの表示\n",
    "    noisy_text = tokenizer.decode(noisy_tokens[0].cpu().tolist())\n",
    "    print(f\"ノイズを加えたテキスト: {noisy_text}\")\n",
    "    \n",
    "    # 逐次的にノイズを除去する簡易版デノイズプロセス\n",
    "    with torch.no_grad():\n",
    "        current_tokens = noisy_tokens.clone()\n",
    "        \n",
    "        for timestep in reversed(range(diffuser.timesteps)):\n",
    "            # マスクされた位置を見つける\n",
    "            mask_positions = (current_tokens == mask_token_id)\n",
    "            \n",
    "            if not mask_positions.any():\n",
    "                break\n",
    "                \n",
    "            # モデルの予測を取得\n",
    "            logits = model(current_tokens)\n",
    "            \n",
    "            # マスクされた位置でのトップk予測を取得\n",
    "            k = 5\n",
    "            topk_probs, topk_indices = torch.topk(logits.softmax(dim=-1), k, dim=-1)\n",
    "            \n",
    "            # マスクごとにトップ1の予測で置き換え\n",
    "            for i in range(current_tokens.size(0)):\n",
    "                for j in range(current_tokens.size(1)):\n",
    "                    if mask_positions[i, j]:\n",
    "                        # ここではトップ1の予測を使用\n",
    "                        current_tokens[i, j] = topk_indices[i, j, 0]\n",
    "                        \n",
    "            # 結果を表示\n",
    "            if timestep % 5 == 0 or timestep == 0:\n",
    "                current_text = tokenizer.decode(current_tokens[0].cpu().tolist())\n",
    "                print(f\"タイムステップ {timestep} での復元: {current_text}\")\n",
    "        \n",
    "        final_text = tokenizer.decode(current_tokens[0].cpu().tolist())\n",
    "        print(f\"\\n最終的な復元テキスト: {final_text}\")\n",
    "else:\n",
    "    print(\"モデルがロードされていないため、推論テストを実行できません\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}